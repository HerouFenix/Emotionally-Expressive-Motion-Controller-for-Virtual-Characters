{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 920,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\noriginal_dim = 27\\nintermediate_dim = 13\\nlatent_dim = 3\\n\\ninputs = keras.Input(shape=(original_dim,))\\nx = layers.Dense(intermediate_dim, activation='relu')(inputs)\\n#x = layers.Dense(intermediate_dim, activation='linear')(inputs)\\n\\nh = layers.Dense(intermediate_dim, activation='relu')(x)\\nz_mean = layers.Dense(latent_dim)(h)\\nz_log_sigma = layers.Dense(latent_dim)(h)\\n\\n\\n\\n# Create encoder\\nencoder = keras.Model(inputs, [z_mean, z_log_sigma, z], name='encoder')\\n\\n# Create decoder\\nlatent_inputs = keras.Input(shape=(latent_dim,), name='z_sampling')\\nx = layers.Dense(intermediate_dim, activation='relu')(latent_inputs)\\n#x = layers.Dense(intermediate_dim, activation='linear')(x)\\n\\noutputs = layers.Dense(original_dim, activation='linear')(x)\\ndecoder = keras.Model(latent_inputs, outputs, name='decoder')\\n\\n# instantiate VAE model\\noutputs = decoder(encoder(inputs)[2])\\nvae = keras.Model(inputs, outputs, name='vae_mlp')\\n\\n\\noptimizer=keras.optimizers.Adam(learning_rate=0.001)\\nvae.compile(optimizer=optimizer)\\n\""
      ]
     },
     "execution_count": 920,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import backend as K\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "\n",
    "\"\"\"\n",
    "original_dim = 27\n",
    "intermediate_dim = 13\n",
    "latent_dim = 3\n",
    "\n",
    "inputs = keras.Input(shape=(original_dim,))\n",
    "x = layers.Dense(intermediate_dim, activation='relu')(inputs)\n",
    "#x = layers.Dense(intermediate_dim, activation='linear')(inputs)\n",
    "\n",
    "h = layers.Dense(intermediate_dim, activation='relu')(x)\n",
    "z_mean = layers.Dense(latent_dim)(h)\n",
    "z_log_sigma = layers.Dense(latent_dim)(h)\n",
    "\n",
    "\n",
    "\n",
    "# Create encoder\n",
    "encoder = keras.Model(inputs, [z_mean, z_log_sigma, z], name='encoder')\n",
    "\n",
    "# Create decoder\n",
    "latent_inputs = keras.Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = layers.Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "#x = layers.Dense(intermediate_dim, activation='linear')(x)\n",
    "\n",
    "outputs = layers.Dense(original_dim, activation='linear')(x)\n",
    "decoder = keras.Model(latent_inputs, outputs, name='decoder')\n",
    "\n",
    "# instantiate VAE model\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "vae = keras.Model(inputs, outputs, name='vae_mlp')\n",
    "\n",
    "\n",
    "optimizer=keras.optimizers.Adam(learning_rate=0.001)\n",
    "vae.compile(optimizer=optimizer)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 970,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dim = 27\n",
    "intermediate_dim = 18\n",
    "intermediate_dim2 = 8\n",
    "latent_dim = 2\n",
    "\n",
    "inputs = keras.Input(shape=(original_dim,))\n",
    "x = layers.Dense(intermediate_dim, activation='relu')(inputs)\n",
    "x2 = layers.Dense(intermediate_dim2, activation='relu')(x)\n",
    "\n",
    "h = layers.Dense(intermediate_dim2, activation='relu')(x2)\n",
    "z_mean = layers.Dense(latent_dim)(h)\n",
    "z_log_sigma = layers.Dense(latent_dim)(h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 971,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_sigma = inputs\n",
    "        \n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        \n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        \n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_sigma) * epsilon\n",
    "\n",
    "z = Sampling()([z_mean, z_log_sigma])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder & VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 972,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create encoder\n",
    "encoder = keras.Model(inputs, [z_mean, z_log_sigma, z], name='encoder')\n",
    "\n",
    "# Create decoder\n",
    "latent_inputs = keras.Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = layers.Dense(intermediate_dim2, activation='relu')(latent_inputs)\n",
    "x = layers.Dense(intermediate_dim, activation='relu')(x)\n",
    "\n",
    "outputs = layers.Dense(original_dim, activation='linear')(x)\n",
    "decoder = keras.Model(latent_inputs, outputs, name='decoder')\n",
    "\n",
    "# instantiate VAE model\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "vae = keras.Model(inputs, outputs, name='vae_mlp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 973,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mse = keras.losses.MeanSquaredError()\n",
    "#reconstruction_loss = mse(inputs, outputs)\n",
    "#reconstruction_loss *= 27\n",
    "reconstruction_loss_factor = 1000\n",
    "reconstruction_loss = tf.keras.backend.mean(tf.keras.backend.square(inputs-outputs))\n",
    "\n",
    "kl_loss = 1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "\n",
    "vae_loss = K.mean((reconstruction_loss_factor*reconstruction_loss) + kl_loss)\n",
    "vae.add_loss(vae_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 974,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Training Samples: 34156\n",
      "No Test Samples: 6028\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('datasets/Fs_B_O_DANCE_WALK_KIN_0.5sec.csv')\n",
    "\n",
    "train_dataset = dataset.sample(frac=0.85, random_state=42)\n",
    "test_dataset = dataset.drop(train_dataset.index)\n",
    "\n",
    "print(\"No Training Samples:\",train_dataset.shape[0])\n",
    "print(\"No Test Samples:\",test_dataset.shape[0])\n",
    "\n",
    "train_dataset = shuffle(train_dataset)\n",
    "test_dataset = shuffle(test_dataset)\n",
    "\n",
    "train_emotions = pd.concat([train_dataset.pop(x) for x in ['EMOTION_P', 'EMOTION_A', 'EMOTION_D']], axis=1)\n",
    "test_emotions = pd.concat([test_dataset.pop(x) for x in ['EMOTION_P', 'EMOTION_A', 'EMOTION_D']], axis=1)\n",
    "\n",
    "\n",
    "train_dataset = np.asarray(train_dataset)\n",
    "test_dataset = np.asarray(test_dataset)\n",
    "\n",
    "x_train = train_dataset.reshape((len(train_dataset), np.prod(train_dataset.shape[1:])))\n",
    "x_test = test_dataset.reshape((len(test_dataset), np.prod(test_dataset.shape[1:])))\n",
    "\n",
    "print(len(x_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 975,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=keras.optimizers.Adam(learning_rate=0.001)\n",
    "vae.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 976,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/256\n",
      "1068/1068 [==============================] - 2s 1ms/step - loss: 118.2572\n",
      "Epoch 2/256\n",
      "1068/1068 [==============================] - 2s 1ms/step - loss: 64.9460\n",
      "Epoch 3/256\n",
      "1068/1068 [==============================] - 2s 1ms/step - loss: 62.8005\n",
      "Epoch 4/256\n",
      "1068/1068 [==============================] - 2s 1ms/step - loss: 60.8321\n",
      "Epoch 5/256\n",
      "1068/1068 [==============================] - 2s 1ms/step - loss: 59.7017\n",
      "Epoch 6/256\n",
      "1068/1068 [==============================] - 2s 1ms/step - loss: 58.7942\n",
      "Epoch 7/256\n",
      "1068/1068 [==============================] - 2s 1ms/step - loss: 58.0960\n",
      "Epoch 8/256\n",
      "1068/1068 [==============================] - 2s 1ms/step - loss: 57.5725\n",
      "Epoch 9/256\n",
      "1068/1068 [==============================] - 2s 1ms/step - loss: 57.1308\n",
      "Epoch 10/256\n",
      "1068/1068 [==============================] - 2s 1ms/step - loss: 56.8623\n",
      "Epoch 11/256\n",
      "1068/1068 [==============================] - 2s 1ms/step - loss: 56.5852\n",
      "Epoch 12/256\n",
      "1068/1068 [==============================] - 2s 1ms/step - loss: 56.2764\n",
      "Epoch 13/256\n",
      "1068/1068 [==============================] - 2s 1ms/step - loss: 56.0251\n",
      "Epoch 14/256\n",
      "1068/1068 [==============================] - 2s 1ms/step - loss: 55.8668\n",
      "Epoch 15/256\n",
      "1026/1068 [===========================>..] - ETA: 0s - loss: 55.6336"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-976-44fccd131614>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m vae.fit(x_train, x_train,\n\u001b[1;32m      2\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m        )\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vae.fit(x_train, x_train,\n",
    "        epochs=1024,\n",
    "        batch_size=32,\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Comparison & Emotion Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 938,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.8208893   0.57730774  0.49820339  0.53456149  0.42063105  0.40112175\n",
      "   0.35034314  0.30452922  0.286151    0.27877407  0.06687857 -0.2828076\n",
      "  -0.23700152  0.92652771  0.57908081  0.09135365  0.22280849  0.31328745\n",
      "   0.49815666  0.50930095  0.4666077   0.44527651  1.93531632  2.26792412\n",
      "   1.16245564  1.10699938  1.00147062]]\n"
     ]
    }
   ],
   "source": [
    "sample = np.asarray(x_test[1])\n",
    "sample = sample.reshape(1,-1)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 939,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.1441323  0.83310235 1.0150403 ]]\n"
     ]
    }
   ],
   "source": [
    "mean, var, generated = encoder.predict(sample)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 940,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.70034444  0.4066999   0.434201    0.42914027  0.43174216  0.4411949\n",
      "   0.34746057  0.3517979   0.2860169   0.2765127  -0.0110607  -0.04170739\n",
      "  -0.10273481  0.93392384  0.3909298   0.12780224  0.16335769  0.70696145\n",
      "   0.82295746  0.42953163  0.4924381   0.4652107   1.7890185   2.2226584\n",
      "   0.8636538   1.1588837   0.90664303]]\n"
     ]
    }
   ],
   "source": [
    "regen = decoder.predict(generated)\n",
    "print(regen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 941,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_p = xgb.XGBRegressor(verbosity=0)\n",
    "model_p.load_model(\"../../emotion_classifier/model_training/models/l2p_dance_model_O.json\")\n",
    "\n",
    "model_a = xgb.XGBRegressor(verbosity=0)\n",
    "model_a.load_model(\"../../emotion_classifier/model_training/models/l2a_dance_model_O.json\")\n",
    "\n",
    "model_d = xgb.XGBRegressor(verbosity=0)\n",
    "model_d.load_model(\"../../emotion_classifier/model_training/models/l2d_dance_model_O.json\")\n",
    "\n",
    "scaler = joblib.load('../../emotion_classifier/model_training/datasets/scalers/standardizers/Fs_B_O_S_DANCE_WALK_KIN_0.5sec.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 942,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real: [[-0.49220023]\n",
      " [ 0.5992359 ]\n",
      " [ 0.88934654]]\n",
      "Predicted: [[0.02798416]\n",
      " [0.16476487]\n",
      " [0.02670545]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diogosilva/.local/lib/python3.7/site-packages/sklearn/base.py:451: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "/home/diogosilva/.local/lib/python3.7/site-packages/sklearn/base.py:451: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n"
     ]
    }
   ],
   "source": [
    "scaled_sample = scaler.transform(sample)\n",
    "\n",
    "real_coordinates = (\n",
    "    model_p.predict(scaled_sample),\n",
    "    model_a.predict(scaled_sample),\n",
    "    model_d.predict(scaled_sample)\n",
    ")\n",
    "\n",
    "scaled_regen = scaler.transform(regen)\n",
    "\n",
    "generated_coordinates = (\n",
    "    model_p.predict(scaled_regen),\n",
    "    model_a.predict(scaled_regen),\n",
    "    model_d.predict(scaled_regen)\n",
    ")\n",
    "\n",
    "\n",
    "print('Real: %s' % np.asarray(real_coordinates))\n",
    "print('Predicted: %s' % np.asarray(generated_coordinates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAE & MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 943,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 944,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7036592  0.4342553  0.37454098 ... 1.8461361  2.5204012  1.7883573 ]\n",
      " [0.7003445  0.40669996 0.434201   ... 0.8636538  1.1588837  0.9066426 ]\n",
      " [0.6201762  0.37764835 0.40436462 ... 0.6113868  0.66184396 0.60663366]\n",
      " ...\n",
      " [0.70893335 0.4264102  0.42156953 ... 0.7753787  0.6809969  0.8165428 ]\n",
      " [0.6548151  0.41043064 0.36380374 ... 1.3698651  2.6179557  1.2896614 ]\n",
      " [0.5788953  0.3778478  0.38853133 ... 1.4338753  1.2692828  2.2325377 ]]\n"
     ]
    }
   ],
   "source": [
    "mean, var, generated = encoder.predict(x_test)\n",
    "regen = decoder.predict(generated)\n",
    "\n",
    "print(regen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 945,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAE: 0.1102329282097022\n",
      "\n",
      "==max_hand_distance==\n",
      "MSE: 0.04548\n",
      "MAE: 0.16615\n",
      "Example [Regen-Real]: 0.7036592 - 0.8239585819804882\n",
      "\n",
      "==avg_l_hand_hip_distance==\n",
      "MSE: 0.01716\n",
      "MAE: 0.10435\n",
      "Example [Regen-Real]: 0.40669996 - 0.5773077437283716\n",
      "\n",
      "==avg_r_hand_hip_distance==\n",
      "MSE: 0.02028\n",
      "MAE: 0.11390\n",
      "Example [Regen-Real]: 0.40436462 - 0.5239518888099561\n",
      "\n",
      "==max_stride_length==\n",
      "MSE: 0.01787\n",
      "MAE: 0.10228\n",
      "Example [Regen-Real]: 0.3245529 - 0.1501770187519039\n",
      "\n",
      "==avg_l_hand_chest_distance==\n",
      "MSE: 0.01053\n",
      "MAE: 0.08080\n",
      "Example [Regen-Real]: 0.40303537 - 0.459039720444281\n",
      "\n",
      "==avg_r_hand_chest_distance==\n",
      "MSE: 0.01036\n",
      "MAE: 0.07894\n",
      "Example [Regen-Real]: 0.41420522 - 0.394466162432962\n",
      "\n",
      "==avg_l_elbow_hip_distance==\n",
      "MSE: 0.00471\n",
      "MAE: 0.05190\n",
      "Example [Regen-Real]: 0.31152064 - 0.3305304711001226\n",
      "\n",
      "==avg_r_elbow_hip_distance==\n",
      "MSE: 0.00482\n",
      "MAE: 0.05167\n",
      "Example [Regen-Real]: 0.34142977 - 0.4395373059793347\n",
      "\n",
      "==avg_chest_pelvis_distance==\n",
      "MSE: 0.00000\n",
      "MAE: 0.00010\n",
      "Example [Regen-Real]: 0.28611562 - 0.2861509999131247\n",
      "\n",
      "==avg_neck_chest_distance==\n",
      "MSE: 0.00001\n",
      "MAE: 0.00184\n",
      "Example [Regen-Real]: 0.2759322 - 0.2681455459318635\n",
      "\n",
      "==avg_neck_rotation_w==\n",
      "MSE: 0.00711\n",
      "MAE: 0.05738\n",
      "Example [Regen-Real]: -0.001631923 - 0.0312736035954421\n",
      "\n",
      "==avg_neck_rotation_x==\n",
      "MSE: 0.06064\n",
      "MAE: 0.16762\n",
      "Example [Regen-Real]: -0.081766985 - -0.319003361368316\n",
      "\n",
      "==avg_neck_rotation_y==\n",
      "MSE: 0.01342\n",
      "MAE: 0.08760\n",
      "Example [Regen-Real]: -0.10067154 - 0.0109341188782541\n",
      "\n",
      "==avg_neck_rotation_z==\n",
      "MSE: 0.00951\n",
      "MAE: 0.05541\n",
      "Example [Regen-Real]: 0.96924937 - 0.9813319958261691\n",
      "\n",
      "==avg_total_body_volume==\n",
      "MSE: 0.01978\n",
      "MAE: 0.10928\n",
      "Example [Regen-Real]: 0.32169273 - 0.2914755625400313\n",
      "\n",
      "==avg_triangle_area_hands_neck==\n",
      "MSE: 0.00350\n",
      "MAE: 0.05020\n",
      "Example [Regen-Real]: 0.115999915 - 0.0508535979541608\n",
      "\n",
      "==avg_triangle_area_feet_hips==\n",
      "MSE: 0.00292\n",
      "MAE: 0.04293\n",
      "Example [Regen-Real]: 0.13250285 - 0.1071839761106768\n",
      "\n",
      "==l_hand_speed==\n",
      "MSE: 0.06470\n",
      "MAE: 0.18750\n",
      "Example [Regen-Real]: 0.7060819 - 0.36698994602351\n",
      "\n",
      "==r_hand_speed==\n",
      "MSE: 0.07034\n",
      "MAE: 0.19815\n",
      "Example [Regen-Real]: 0.1198219 - 0.0734302741289827\n",
      "\n",
      "==l_foot_speed==\n",
      "MSE: 0.04857\n",
      "MAE: 0.15111\n",
      "Example [Regen-Real]: 0.14011398 - 0.1014504693783688\n",
      "\n",
      "==r_foot_speed==\n",
      "MSE: 0.04863\n",
      "MAE: 0.14792\n",
      "Example [Regen-Real]: 0.33214092 - 0.5474752774576424\n",
      "\n",
      "==neck_speed==\n",
      "MSE: 0.04727\n",
      "MAE: 0.15797\n",
      "Example [Regen-Real]: 0.68172324 - 0.6006519953411401\n",
      "\n",
      "==l_hand_acceleration_magnitude==\n",
      "MSE: 0.03490\n",
      "MAE: 0.12057\n",
      "Example [Regen-Real]: 0.3623641 - 0.3887060061752223\n",
      "\n",
      "==r_hand_acceleration_magnitude==\n",
      "MSE: 0.04704\n",
      "MAE: 0.14917\n",
      "Example [Regen-Real]: 2.8951528 - 3.0758492024408177\n",
      "\n",
      "==l_foot_acceleration_magnitude==\n",
      "MSE: 0.08628\n",
      "MAE: 0.19524\n",
      "Example [Regen-Real]: 0.64996207 - 0.3557611112786494\n",
      "\n",
      "==r_foot_acceleration_magnitude==\n",
      "MSE: 0.05506\n",
      "MAE: 0.16386\n",
      "Example [Regen-Real]: 1.0137736 - 1.1904152197787028\n",
      "\n",
      "==neck_acceleration_magnitude==\n",
      "MSE: 0.07248\n",
      "MAE: 0.18244\n",
      "Example [Regen-Real]: 0.8240999 - 0.8697028166863233\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mae_errors = mean_absolute_error(x_test, regen, multioutput='raw_values')\n",
    "mse_errors = mean_squared_error(x_test, regen, multioutput='raw_values')\n",
    "\n",
    "features = [\"max_hand_distance\",\n",
    "          \"avg_l_hand_hip_distance\",\n",
    "          \"avg_r_hand_hip_distance\",\n",
    "          \"max_stride_length\",\n",
    "          \"avg_l_hand_chest_distance\",\n",
    "          \"avg_r_hand_chest_distance\",\n",
    "          \"avg_l_elbow_hip_distance\",\n",
    "          \"avg_r_elbow_hip_distance\",\n",
    "          \"avg_chest_pelvis_distance\",\n",
    "          \"avg_neck_chest_distance\",\n",
    "          \"avg_neck_rotation_w\", \"avg_neck_rotation_x\", \"avg_neck_rotation_y\", \"avg_neck_rotation_z\",\n",
    "          \"avg_total_body_volume\",\n",
    "          \"avg_triangle_area_hands_neck\",\n",
    "          \"avg_triangle_area_feet_hips\",\n",
    "          \n",
    "          \"l_hand_speed\",\n",
    "          \"r_hand_speed\",\n",
    "          \"l_foot_speed\",\n",
    "          \"r_foot_speed\",\n",
    "          \"neck_speed\",\n",
    "          \n",
    "          \"l_hand_acceleration_magnitude\",\n",
    "          \"r_hand_acceleration_magnitude\",\n",
    "          \"l_foot_acceleration_magnitude\",\n",
    "          \"r_foot_acceleration_magnitude\",\n",
    "          \"neck_acceleration_magnitude\",\n",
    "         ]\n",
    "\n",
    "print(\"Overall MAE: \" + str(mean_absolute_error(x_test, regen)))\n",
    "\n",
    "print()\n",
    "for i in range(len(mse_errors)):\n",
    "    print(\"==\" + features[i] + \"==\")\n",
    "    print(\"MSE: %.5f\" % mse_errors[i])\n",
    "    print(\"MAE: %.5f\" % mae_errors[i])\n",
    "    print(\"Example [Regen-Real]: \" + str(regen[i][i]) + \" - \" + str(x_test[i][i]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PAD to Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 946,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "xgb.set_config(verbosity=0)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "import math\n",
    "import joblib\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 947,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_p = xgb.XGBRegressor(verbosity=0)\n",
    "model_p.load_model(\"../../emotion_classifier/model_training/models/l2p_dance_model_O.json\")\n",
    "\n",
    "model_a = xgb.XGBRegressor(verbosity=0)\n",
    "model_a.load_model(\"../../emotion_classifier/model_training/models/l2a_dance_model_O.json\")\n",
    "\n",
    "model_d = xgb.XGBRegressor(verbosity=0)\n",
    "model_d.load_model(\"../../emotion_classifier/model_training/models/l2d_dance_model_O.json\")\n",
    "\n",
    "scaler = joblib.load('../../emotion_classifier/model_training/datasets/scalers/standardizers/Fs_B_O_S_DANCE_WALK_KIN_0.5sec.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 948,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Training Samples: 34156\n",
      "No Test Samples: 6028\n"
     ]
    }
   ],
   "source": [
    "# X -> Predicted emotions of LMA features\n",
    "# y -> Latent Space of LMA features\n",
    "# Good Error: Predicted emotion of LMA features generated from latent space == Predicted emotions of original LMA features\n",
    "\n",
    "dataset = pd.read_csv('../../emotion_classifier/model_training/datasets/Fs_B_O_S_DANCE_WALK_KIN_0.5sec.csv')\n",
    "\n",
    "train_dataset = dataset.sample(frac=0.85, random_state=42)\n",
    "test_dataset = dataset.drop(train_dataset.index)\n",
    "\n",
    "print(\"No Training Samples:\",train_dataset.shape[0])\n",
    "print(\"No Test Samples:\",test_dataset.shape[0])\n",
    "\n",
    "train_lma = train_dataset.copy()\n",
    "test_lma = test_dataset.copy()\n",
    "\n",
    "train_emotions = pd.concat([train_lma.pop(x) for x in ['EMOTION_P', 'EMOTION_A', 'EMOTION_D']], axis=1)\n",
    "test_emotions = pd.concat([test_lma.pop(x) for x in ['EMOTION_P', 'EMOTION_A', 'EMOTION_D']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 949,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMOTION_P</th>\n",
       "      <th>EMOTION_A</th>\n",
       "      <th>EMOTION_D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.596379</td>\n",
       "      <td>0.498261</td>\n",
       "      <td>0.199826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.699091</td>\n",
       "      <td>0.199438</td>\n",
       "      <td>0.199085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.586425</td>\n",
       "      <td>-0.288315</td>\n",
       "      <td>-0.289255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.350705</td>\n",
       "      <td>0.698464</td>\n",
       "      <td>-0.799689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.351387</td>\n",
       "      <td>0.699263</td>\n",
       "      <td>-0.800516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   EMOTION_P  EMOTION_A  EMOTION_D\n",
       "0   0.596379   0.498261   0.199826\n",
       "1   0.699091   0.199438   0.199085\n",
       "2  -0.586425  -0.288315  -0.289255\n",
       "3  -0.350705   0.698464  -0.799689\n",
       "4  -0.351387   0.699263  -0.800516"
      ]
     },
     "execution_count": 949,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_p = model_p.predict(train_lma)\n",
    "train_X_a = model_a.predict(train_lma)\n",
    "train_X_d = model_d.predict(train_lma)\n",
    "\n",
    "rows = []\n",
    "for i in range(len(train_lma)):\n",
    "    rows.append([train_X_p[i], train_X_a[i], train_X_d[i]])\n",
    "\n",
    "train_X = pd.DataFrame(rows, columns=[\n",
    "            \"EMOTION_P\", \"EMOTION_A\", \"EMOTION_D\"\n",
    "         ])\n",
    "\n",
    "train_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 950,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMOTION_P</th>\n",
       "      <th>EMOTION_A</th>\n",
       "      <th>EMOTION_D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.499540</td>\n",
       "      <td>0.599887</td>\n",
       "      <td>0.896598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.599478</td>\n",
       "      <td>0.496702</td>\n",
       "      <td>0.201141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.581306</td>\n",
       "      <td>-0.292542</td>\n",
       "      <td>-0.287728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.602087</td>\n",
       "      <td>-0.301238</td>\n",
       "      <td>-0.288370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.595353</td>\n",
       "      <td>-0.301161</td>\n",
       "      <td>-0.302442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   EMOTION_P  EMOTION_A  EMOTION_D\n",
       "0  -0.499540   0.599887   0.896598\n",
       "1   0.599478   0.496702   0.201141\n",
       "2  -0.581306  -0.292542  -0.287728\n",
       "3  -0.602087  -0.301238  -0.288370\n",
       "4  -0.595353  -0.301161  -0.302442"
      ]
     },
     "execution_count": 950,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X_p = model_p.predict(test_lma)\n",
    "test_X_a = model_a.predict(test_lma)\n",
    "test_X_d = model_d.predict(test_lma)\n",
    "\n",
    "rows = []\n",
    "for i in range(len(test_lma)):\n",
    "    rows.append([test_X_p[i], test_X_a[i], test_X_d[i]])\n",
    "\n",
    "test_X = pd.DataFrame(rows, columns=[\n",
    "            \"EMOTION_P\", \"EMOTION_A\", \"EMOTION_D\"\n",
    "         ])\n",
    "\n",
    "test_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 951,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LATENT_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.751822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.582938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.040619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.569951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.689253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LATENT_1\n",
       "0  0.751822\n",
       "1  0.582938\n",
       "2  1.040619\n",
       "3  0.569951\n",
       "4  0.689253"
      ]
     },
     "execution_count": 951,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, var, latent_space = encoder.predict(train_lma)\n",
    "\n",
    "rows = []\n",
    "for i in range(len(latent_space)):\n",
    "    rows.append([latent_space[i][0], latent_space[i][1], latent_space[i][2]])\n",
    "    \n",
    "train_y = pd.DataFrame(rows, columns=[\n",
    "            \"LATENT_1\", \"LATENT_2\" #, \"LATENT_3\"\n",
    "         ])\n",
    "\n",
    "train_y_1 = pd.concat([train_y.pop(x) for x in ['LATENT_1']], axis=1)\n",
    "train_y_2 = pd.concat([train_y.pop(x) for x in ['LATENT_2']], axis=1)\n",
    "#train_y_3 = pd.concat([train_y.pop(x) for x in ['LATENT_3']], axis=1)\n",
    "\n",
    "train_y_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 952,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LATENT_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.194246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.949085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.248255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.008567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.994421</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LATENT_1\n",
       "0  1.194246\n",
       "1  0.949085\n",
       "2  1.248255\n",
       "3  1.008567\n",
       "4  0.994421"
      ]
     },
     "execution_count": 952,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean, var, latent_space = encoder.predict(test_lma)\n",
    "\n",
    "rows = []\n",
    "for i in range(len(latent_space)):\n",
    "    rows.append([latent_space[i][0], latent_space[i][1], latent_space[i][2]])\n",
    "    \n",
    "test_y = pd.DataFrame(rows, columns=[\n",
    "            \"LATENT_1\", \"LATENT_2\" #, \"LATENT_3\"\n",
    "         ])\n",
    "\n",
    "test_y_1 = pd.concat([test_y.pop(x) for x in ['LATENT_1']], axis=1)\n",
    "test_y_2 = pd.concat([test_y.pop(x) for x in ['LATENT_2']], axis=1)\n",
    "#test_y_3 = pd.concat([test_y.pop(x) for x in ['LATENT_3']], axis=1)\n",
    "\n",
    "test_y_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 953,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = xgb.XGBRegressor(\n",
    "                    n_estimators=1500, learning_rate=0.05, max_depth=10, min_child_weight=5, \n",
    "                    reg_alpha=0.1, reg_lambda=1, gamma=0.0,\n",
    "                    subsample=0.75, colsample_bytree=0.75, objective=\"reg:squarederror\",\n",
    "                    tree_method='gpu_hist'\n",
    "                )\n",
    "\n",
    "model_2 = xgb.XGBRegressor(\n",
    "                    n_estimators=1500, learning_rate=0.05, max_depth=10, min_child_weight=5, \n",
    "                    reg_alpha=0.1, reg_lambda=1, gamma=0.0,\n",
    "                    subsample=0.75, colsample_bytree=0.75, objective=\"reg:squarederror\",\n",
    "                    tree_method='gpu_hist'\n",
    "                )\n",
    "\n",
    "#model_3 = xgb.XGBRegressor(\n",
    "#                    n_estimators=1500, learning_rate=0.05, max_depth=10, min_child_weight=5, \n",
    "#                    reg_alpha=0.1, reg_lambda=1, gamma=0.0,\n",
    "#                    subsample=0.75, colsample_bytree=0.75, objective=\"reg:squarederror\",\n",
    "#                    tree_method='gpu_hist'\n",
    "#                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 954,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=0.75,\n",
       "             enable_categorical=False, gamma=0.0, gpu_id=0,\n",
       "             importance_type=None, interaction_constraints='',\n",
       "             learning_rate=0.05, max_delta_step=0, max_depth=10,\n",
       "             min_child_weight=5, missing=nan, monotone_constraints='()',\n",
       "             n_estimators=1500, n_jobs=12, num_parallel_tree=1,\n",
       "             predictor='auto', random_state=0, reg_alpha=0.1, reg_lambda=1,\n",
       "             scale_pos_weight=1, subsample=0.75, tree_method='gpu_hist',\n",
       "             validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 954,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.fit(train_X, train_y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 955,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=0.75,\n",
       "             enable_categorical=False, gamma=0.0, gpu_id=0,\n",
       "             importance_type=None, interaction_constraints='',\n",
       "             learning_rate=0.05, max_delta_step=0, max_depth=10,\n",
       "             min_child_weight=5, missing=nan, monotone_constraints='()',\n",
       "             n_estimators=1500, n_jobs=12, num_parallel_tree=1,\n",
       "             predictor='auto', random_state=0, reg_alpha=0.1, reg_lambda=1,\n",
       "             scale_pos_weight=1, subsample=0.75, tree_method='gpu_hist',\n",
       "             validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 955,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.fit(train_X, train_y_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 956,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=0.75,\n",
       "             enable_categorical=False, gamma=0.0, gpu_id=0,\n",
       "             importance_type=None, interaction_constraints='',\n",
       "             learning_rate=0.05, max_delta_step=0, max_depth=10,\n",
       "             min_child_weight=5, missing=nan, monotone_constraints='()',\n",
       "             n_estimators=1500, n_jobs=12, num_parallel_tree=1,\n",
       "             predictor='auto', random_state=0, reg_alpha=0.1, reg_lambda=1,\n",
       "             scale_pos_weight=1, subsample=0.75, tree_method='gpu_hist',\n",
       "             validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 956,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model_3.fit(train_X, train_y_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 957,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent 1\n",
      "Training score:  0.5749038956031831\n",
      "\n",
      "Latent 2\n",
      "Training score:  0.5931620956821015\n",
      "\n",
      "Latent 3\n",
      "Training score:  0.5885978770571259\n"
     ]
    }
   ],
   "source": [
    "score = model_1.score(train_X, train_y_1)  \n",
    "\n",
    "print(\"Latent 1\")\n",
    "print(\"Training score: \", score)\n",
    "\n",
    "print()\n",
    "\n",
    "score = model_2.score(train_X, train_y_2)  \n",
    "\n",
    "print(\"Latent 2\")\n",
    "print(\"Training score: \", score)\n",
    "\n",
    "score = model_3.score(train_X, train_y_3)  \n",
    "\n",
    "print()\n",
    "\n",
    "#print(\"Latent 3\")\n",
    "#print(\"Training score: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 958,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent 1\n",
      "MSE: 0.19\n",
      "MAE: 0.31\n",
      "\n",
      "Latent 2\n",
      "MSE: 0.08\n",
      "MAE: 0.20\n",
      "\n",
      "Latent 3\n",
      "MSE: 0.15\n",
      "MAE: 0.27\n"
     ]
    }
   ],
   "source": [
    "pred_y_1 = model_1.predict(test_X)\n",
    "mse = mean_squared_error(test_y_1, pred_y_1)\n",
    "mae = mean_absolute_error(test_y_1, pred_y_1)\n",
    "print(\"Latent 1\")\n",
    "print(\"MSE: %.2f\" % mse)\n",
    "print(\"MAE: %.2f\" % mae)\n",
    "\n",
    "\n",
    "pred_y_2 = model_2.predict(test_X)\n",
    "mse = mean_squared_error(test_y_2, pred_y_2)\n",
    "mae = mean_absolute_error(test_y_2, pred_y_2)\n",
    "print(\"\\nLatent 2\")\n",
    "print(\"MSE: %.2f\" % mse)\n",
    "print(\"MAE: %.2f\" % mae)\n",
    "\n",
    "\n",
    "#pred_y_3 = model_3.predict(test_X)\n",
    "#mse = mean_squared_error(test_y_3, pred_y_3)\n",
    "#mae = mean_absolute_error(test_y_3, pred_y_3)\n",
    "#print(\"\\nLatent 3\")\n",
    "#print(\"MSE: %.2f\" % mse)\n",
    "#print(\"MAE: %.2f\" % mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 969,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real: [-0.5046277   0.5842134   0.86978376]\n",
      "Predicted: [-0.27994117 -0.0327893  -0.37819585]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diogosilva/.local/lib/python3.7/site-packages/sklearn/base.py:451: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n"
     ]
    }
   ],
   "source": [
    "index = 10\n",
    "\n",
    "# Real -> PAD coordinates\n",
    "#   - get latent space\n",
    "#   - convert from latent to LMA features\n",
    "#   - standardize\n",
    "#   - predict coordinates of generated features\n",
    "\n",
    "sample = np.asarray([test_X.iloc[index]])\n",
    "latent = np.asarray([[model_1.predict(sample)[0], model_2.predict(sample)[0]]])#, model_3.predict(sample)[0]]])\n",
    "\n",
    "\n",
    "generated_lma = decoder.predict(latent)\n",
    "\n",
    "\n",
    "scaled_gen = scaler.transform(generated_lma)\n",
    "\n",
    "generated_coord = (\n",
    "    model_p.predict(scaled_gen)[0],\n",
    "    model_a.predict(scaled_gen)[0],\n",
    "    model_d.predict(scaled_gen)[0]\n",
    ")\n",
    "\n",
    "print('Real: %s' % sample[0])\n",
    "print('Predicted: %s' % np.asarray(generated_coord))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 939,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A parameter grid for XGBoost\n",
    "# https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "params = {\n",
    "        'eta': [0.01, 0.05, 0.1],\n",
    "        'min_child_weight': [1, 5, 11, 21],\n",
    "        'max_depth': [3, 6, 10, 15],\n",
    "        'gamma': [0, 0.001, 0.01],\n",
    "        'subsample': [0.75, 1],\n",
    "        'colsample_bytree': [0.75, 1],\n",
    "        'lambda': [1, 1.25],\n",
    "        'alpha': [0.0, 0.25]\n",
    "        }\n",
    "\n",
    "n_iter = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 940,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = xgb.XGBRegressor(\n",
    "                    n_estimators=1500,\n",
    "                    objective=\"reg:squarederror\",\n",
    "                    tree_method='gpu_hist'\n",
    "                )\n",
    "\n",
    "model_2 = xgb.XGBRegressor(\n",
    "                    n_estimators=1500,\n",
    "                    objective=\"reg:squarederror\",\n",
    "                    tree_method='gpu_hist'\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 941,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV took 2863.87 seconds parameter settings.\n"
     ]
    }
   ],
   "source": [
    "# Latent 1\n",
    "# run randomized search\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "random_search_1 = RandomizedSearchCV(model_1, param_distributions=params,\n",
    "                               cv=kfold, scoring='neg_mean_squared_error', n_iter = n_iter)\n",
    "\n",
    "start = time.time()\n",
    "random_search.fit(train_X, train_y_1)\n",
    "\n",
    "\n",
    "print(\"GridSearchCV took %.2f seconds\"\n",
    "      \" parameter settings.\" % ((time.time() - start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent 2\n",
    "# run randomized search\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "random_search_2 = RandomizedSearchCV(model_2, param_distributions=params,\n",
    "                               cv=kfold, scoring='neg_mean_squared_error', n_iter = n_iter)\n",
    "\n",
    "start = time.time()\n",
    "random_search.fit(train_X, train_y_2)\n",
    "\n",
    "\n",
    "print(\"GridSearchCV took %.2f seconds\"\n",
    "      \" parameter settings.\" % ((time.time() - start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 942,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'objective': 'reg:squarederror', 'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 1, 'enable_categorical': False, 'gamma': 0.001, 'gpu_id': 0, 'importance_type': None, 'interaction_constraints': '', 'learning_rate': 0.00999999978, 'max_delta_step': 0, 'max_depth': 3, 'min_child_weight': 5, 'missing': nan, 'monotone_constraints': '()', 'n_estimators': 1500, 'n_jobs': 12, 'num_parallel_tree': 1, 'predictor': 'auto', 'random_state': 0, 'reg_alpha': 0.25, 'reg_lambda': 1, 'scale_pos_weight': 1, 'subsample': 0.75, 'tree_method': 'gpu_hist', 'validate_parameters': 1, 'verbosity': None, 'lambda': 1, 'eta': 0.01, 'alpha': 0.25}\n"
     ]
    }
   ],
   "source": [
    "best_regressor_1 = random_search_1.best_estimator_\n",
    "\n",
    "print(best_regressor_1.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_regressor_1 = random_search_1.best_estimator_\n",
    "\n",
    "print(best_regressor_1.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 943,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.98\n",
      "MAE: 0.78\n"
     ]
    }
   ],
   "source": [
    "pred_y = best_regressor.predict(reg_test_X)\n",
    "mse = mean_squared_error(reg_test_Y, pred_y)\n",
    "mae = mean_absolute_error(reg_test_Y, pred_y)\n",
    "\n",
    "print(\"MSE: %.2f\" % mse)\n",
    "print(\"MAE: %.2f\" % mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
