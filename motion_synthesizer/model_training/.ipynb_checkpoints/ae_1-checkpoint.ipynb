{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import backend as K\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from keras import regularizers\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import time\n",
    "from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold\n",
    "import sklearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Training Samples: 34156\n",
      "No Test Samples: 6028\n",
      "27\n",
      "[ 0.34312904  0.51705042  0.51322024  0.56975248  0.30434335  0.29322879\n",
      "  0.29375089  0.27150061  0.286151    0.27883848  0.01622985 -0.17582348\n",
      " -0.01717104  0.98329522  0.40441222  0.02458114  0.23367192  0.9586529\n",
      "  0.91985299  0.47343646  0.26013893  0.33075202  1.85855878  1.65382635\n",
      "  1.03435026  0.79464071  0.74038627]\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('datasets/Fs_B_O_DANCE_WALK_KIN_0.5sec.csv')\n",
    "\n",
    "train_dataset = dataset.sample(frac=0.85, random_state=42)\n",
    "test_dataset = dataset.drop(train_dataset.index)\n",
    "\n",
    "print(\"No Training Samples:\",train_dataset.shape[0])\n",
    "print(\"No Test Samples:\",test_dataset.shape[0])\n",
    "\n",
    "train_dataset = shuffle(train_dataset)\n",
    "test_dataset = shuffle(test_dataset)\n",
    "\n",
    "train_emotions = pd.concat([train_dataset.pop(x) for x in ['EMOTION_P', 'EMOTION_A', 'EMOTION_D']], axis=1)\n",
    "test_emotions = pd.concat([test_dataset.pop(x) for x in ['EMOTION_P', 'EMOTION_A', 'EMOTION_D']], axis=1)\n",
    "\n",
    "\n",
    "train_dataset = np.asarray(train_dataset)\n",
    "test_dataset = np.asarray(test_dataset)\n",
    "\n",
    "x_train = train_dataset.reshape((len(train_dataset), np.prod(train_dataset.shape[1:])))\n",
    "x_test = test_dataset.reshape((len(test_dataset), np.prod(test_dataset.shape[1:])))\n",
    "\n",
    "\n",
    "input_shape = len(x_train[0])\n",
    "print(len(x_train[0]))\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_dim = 3  # 3 floats -> compression of factor 3, assuming the input is 27 floats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our input image\n",
    "input_lma = keras.Input(shape=(input_shape,))\n",
    "\n",
    "# encoder hidden layers\n",
    "#\n",
    "\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = layers.Dense(encoding_dim, activation='relu')(input_lma)\n",
    "\n",
    "# decoder hidden layers\n",
    "#\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = layers.Dense(input_shape, activation='linear')(encoded)\n",
    "\n",
    "# This model maps an input to its reconstruction\n",
    "autoencoder = keras.Model(input_lma, decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model maps an input to its encoded representation\n",
    "encoder = keras.Model(input_lma, encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our encoded (3-dimensional) input\n",
    "encoded_input = keras.Input(shape=(encoding_dim,))\n",
    "\n",
    "# Retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "\n",
    "# Create the decoder model\n",
    "decoder = keras.Model(encoded_input, decoder_layer(encoded_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer=keras.optimizers.Adam(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/256\n",
      "134/134 [==============================] - 0s 2ms/step - loss: 0.4542 - val_loss: 0.3846\n",
      "Epoch 2/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.3376 - val_loss: 0.2952\n",
      "Epoch 3/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.2569 - val_loss: 0.2179\n",
      "Epoch 4/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1990 - val_loss: 0.1869\n",
      "Epoch 5/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1767 - val_loss: 0.1701\n",
      "Epoch 6/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1647 - val_loss: 0.1622\n",
      "Epoch 7/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1586 - val_loss: 0.1573\n",
      "Epoch 8/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1540 - val_loss: 0.1528\n",
      "Epoch 9/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1493 - val_loss: 0.1479\n",
      "Epoch 10/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1447 - val_loss: 0.1434\n",
      "Epoch 11/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1408 - val_loss: 0.1401\n",
      "Epoch 12/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1383 - val_loss: 0.1383\n",
      "Epoch 13/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1371 - val_loss: 0.1376\n",
      "Epoch 14/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1365 - val_loss: 0.1371\n",
      "Epoch 15/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1361 - val_loss: 0.1368\n",
      "Epoch 16/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1358 - val_loss: 0.1366\n",
      "Epoch 17/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1356 - val_loss: 0.1364\n",
      "Epoch 18/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1355 - val_loss: 0.1363\n",
      "Epoch 19/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1354 - val_loss: 0.1362\n",
      "Epoch 20/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1353 - val_loss: 0.1361\n",
      "Epoch 21/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1353 - val_loss: 0.1361\n",
      "Epoch 22/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1353 - val_loss: 0.1362\n",
      "Epoch 23/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1352 - val_loss: 0.1361\n",
      "Epoch 24/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1352 - val_loss: 0.1361\n",
      "Epoch 25/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1352 - val_loss: 0.1361\n",
      "Epoch 26/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1352 - val_loss: 0.1361\n",
      "Epoch 27/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1352 - val_loss: 0.1361\n",
      "Epoch 28/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1352 - val_loss: 0.1361\n",
      "Epoch 29/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1352 - val_loss: 0.1361\n",
      "Epoch 30/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1352 - val_loss: 0.1360\n",
      "Epoch 31/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1352 - val_loss: 0.1361\n",
      "Epoch 32/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1352 - val_loss: 0.1360\n",
      "Epoch 33/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1352 - val_loss: 0.1360\n",
      "Epoch 34/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1352 - val_loss: 0.1360\n",
      "Epoch 35/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1351 - val_loss: 0.1360\n",
      "Epoch 36/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1351 - val_loss: 0.1360\n",
      "Epoch 37/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1351 - val_loss: 0.1360\n",
      "Epoch 38/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1351 - val_loss: 0.1360\n",
      "Epoch 39/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1351 - val_loss: 0.1360\n",
      "Epoch 40/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1351 - val_loss: 0.1360\n",
      "Epoch 41/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1350 - val_loss: 0.1359\n",
      "Epoch 42/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1350 - val_loss: 0.1359\n",
      "Epoch 43/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1350 - val_loss: 0.1358\n",
      "Epoch 44/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1349 - val_loss: 0.1358\n",
      "Epoch 45/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1348 - val_loss: 0.1356\n",
      "Epoch 46/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1347 - val_loss: 0.1355\n",
      "Epoch 47/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1345 - val_loss: 0.1353\n",
      "Epoch 48/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1343 - val_loss: 0.1352\n",
      "Epoch 49/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1342 - val_loss: 0.1350\n",
      "Epoch 50/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1341 - val_loss: 0.1350\n",
      "Epoch 51/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1340 - val_loss: 0.1351\n",
      "Epoch 52/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1340 - val_loss: 0.1349\n",
      "Epoch 53/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1340 - val_loss: 0.1349\n",
      "Epoch 54/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1340 - val_loss: 0.1350\n",
      "Epoch 55/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 56/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 57/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 58/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 59/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 60/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 61/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 62/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 63/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 64/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 65/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1350\n",
      "Epoch 66/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 67/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1348\n",
      "Epoch 68/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 69/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 70/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1348\n",
      "Epoch 71/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 72/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 73/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1348\n",
      "Epoch 74/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 75/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 76/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 77/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 78/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 79/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 80/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 81/256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 82/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 83/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 84/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 85/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 86/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 87/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 88/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1348\n",
      "Epoch 89/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 90/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 91/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1348\n",
      "Epoch 92/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 93/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 94/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1348\n",
      "Epoch 95/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 96/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 97/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 98/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 99/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 100/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 101/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 102/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 103/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 104/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 105/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 106/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 107/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 108/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 109/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 110/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1348\n",
      "Epoch 111/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 112/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 113/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 114/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 115/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 116/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1348\n",
      "Epoch 117/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 118/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 119/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 120/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 121/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1350\n",
      "Epoch 122/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 123/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 124/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 125/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 126/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 127/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 128/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 129/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1348\n",
      "Epoch 130/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1348\n",
      "Epoch 131/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 132/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1348\n",
      "Epoch 133/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 134/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 135/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1348\n",
      "Epoch 136/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1348\n",
      "Epoch 137/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1348\n",
      "Epoch 138/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 139/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 140/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 141/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 142/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1350\n",
      "Epoch 143/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 144/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 145/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 146/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1348\n",
      "Epoch 147/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 148/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 149/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 150/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 151/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 152/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 153/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 154/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1348\n",
      "Epoch 155/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 156/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 157/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 158/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 159/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 160/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 162/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 163/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 164/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1348\n",
      "Epoch 165/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1348\n",
      "Epoch 166/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 167/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 168/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1348\n",
      "Epoch 169/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 170/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 171/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 172/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 173/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 174/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 175/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 176/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1348\n",
      "Epoch 177/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 178/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 179/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1348\n",
      "Epoch 180/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 181/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 182/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 183/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1348\n",
      "Epoch 184/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 185/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 186/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1348\n",
      "Epoch 187/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 188/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1348\n",
      "Epoch 189/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 190/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 191/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 192/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 193/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 194/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 195/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 196/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1348\n",
      "Epoch 197/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 198/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 199/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1348\n",
      "Epoch 200/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 201/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 202/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 203/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 204/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 205/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 206/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 207/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 208/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 209/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 210/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 211/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 212/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1348\n",
      "Epoch 213/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1348\n",
      "Epoch 214/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 215/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 216/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 217/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 218/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 219/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 220/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 221/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 222/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 223/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1348\n",
      "Epoch 224/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 225/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 226/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 227/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1348\n",
      "Epoch 228/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1348\n",
      "Epoch 229/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 230/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 231/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 232/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 233/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 234/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1348\n",
      "Epoch 235/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 236/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 237/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1348\n",
      "Epoch 238/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1348\n",
      "Epoch 239/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 240/256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1348\n",
      "Epoch 241/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 242/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 243/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 244/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 245/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 246/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 247/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1348\n",
      "Epoch 248/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 249/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 250/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 251/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 252/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 253/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1348\n",
      "Epoch 254/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1348\n",
      "Epoch 255/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1349\n",
      "Epoch 256/256\n",
      "134/134 [==============================] - 0s 1ms/step - loss: 0.1339 - val_loss: 0.1348\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe0945e0090>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=256,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_lma = encoder.predict(x_test)\n",
    "decoded_lma = decoder.predict(encoded_lma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall MAE: 0.13484912986508546\n",
      "\n",
      "==max_hand_distance==\n",
      "MSE: 0.06153\n",
      "MAE: 0.18874\n",
      "Example [Regen-Real]: 0.43362844 - 0.553401847314621\n",
      "\n",
      "==avg_l_hand_hip_distance==\n",
      "MSE: 0.02013\n",
      "MAE: 0.10002\n",
      "Example [Regen-Real]: 0.2997385 - 0.242314395568318\n",
      "\n",
      "==avg_r_hand_hip_distance==\n",
      "MSE: 0.02682\n",
      "MAE: 0.11822\n",
      "Example [Regen-Real]: 0.28957313 - 0.4773429846173097\n",
      "\n",
      "==max_stride_length==\n",
      "MSE: 0.01848\n",
      "MAE: 0.10387\n",
      "Example [Regen-Real]: 0.37796444 - 0.2855529730041133\n",
      "\n",
      "==avg_l_hand_chest_distance==\n",
      "MSE: 0.01220\n",
      "MAE: 0.07884\n",
      "Example [Regen-Real]: 0.44479546 - 0.3588483864545449\n",
      "\n",
      "==avg_r_hand_chest_distance==\n",
      "MSE: 0.01205\n",
      "MAE: 0.07885\n",
      "Example [Regen-Real]: 0.4422291 - 0.2553127404691596\n",
      "\n",
      "==avg_l_elbow_hip_distance==\n",
      "MSE: 0.00527\n",
      "MAE: 0.05264\n",
      "Example [Regen-Real]: 0.32883224 - 0.3657583855167941\n",
      "\n",
      "==avg_r_elbow_hip_distance==\n",
      "MSE: 0.00564\n",
      "MAE: 0.05466\n",
      "Example [Regen-Real]: 0.33765796 - 0.343559881659532\n",
      "\n",
      "==avg_chest_pelvis_distance==\n",
      "MSE: 0.00000\n",
      "MAE: 0.00008\n",
      "Example [Regen-Real]: 0.28603727 - 0.2861510002613038\n",
      "\n",
      "==avg_neck_chest_distance==\n",
      "MSE: 0.00001\n",
      "MAE: 0.00115\n",
      "Example [Regen-Real]: 0.2787022 - 0.2788649626924828\n",
      "\n",
      "==avg_neck_rotation_w==\n",
      "MSE: 0.00731\n",
      "MAE: 0.05762\n",
      "Example [Regen-Real]: 0.0057350527 - -0.0655996847314279\n",
      "\n",
      "==avg_neck_rotation_x==\n",
      "MSE: 0.06158\n",
      "MAE: 0.16225\n",
      "Example [Regen-Real]: -0.11675787 - 0.061903247831064\n",
      "\n",
      "==avg_neck_rotation_y==\n",
      "MSE: 0.01388\n",
      "MAE: 0.08389\n",
      "Example [Regen-Real]: -0.05142872 - -0.0602257558290557\n",
      "\n",
      "==avg_neck_rotation_z==\n",
      "MSE: 0.01075\n",
      "MAE: 0.04873\n",
      "Example [Regen-Real]: 0.94493514 - 0.9317145625680564\n",
      "\n",
      "==avg_total_body_volume==\n",
      "MSE: 0.02340\n",
      "MAE: 0.11272\n",
      "Example [Regen-Real]: 0.33130127 - 0.5464115100947289\n",
      "\n",
      "==avg_triangle_area_hands_neck==\n",
      "MSE: 0.00365\n",
      "MAE: 0.05101\n",
      "Example [Regen-Real]: 0.11686149 - 0.1796844480860194\n",
      "\n",
      "==avg_triangle_area_feet_hips==\n",
      "MSE: 0.00298\n",
      "MAE: 0.04319\n",
      "Example [Regen-Real]: 0.14006802 - 0.1902703862990916\n",
      "\n",
      "==l_hand_speed==\n",
      "MSE: 0.07965\n",
      "MAE: 0.20446\n",
      "Example [Regen-Real]: 0.053938996 - 0.0644375493824879\n",
      "\n",
      "==r_hand_speed==\n",
      "MSE: 0.11887\n",
      "MAE: 0.24378\n",
      "Example [Regen-Real]: 0.3983659 - 0.2316956843879231\n",
      "\n",
      "==l_foot_speed==\n",
      "MSE: 0.05063\n",
      "MAE: 0.14919\n",
      "Example [Regen-Real]: 0.051761523 - 0.0544778283137327\n",
      "\n",
      "==r_foot_speed==\n",
      "MSE: 0.05348\n",
      "MAE: 0.15285\n",
      "Example [Regen-Real]: 0.74654347 - 0.3283815495108355\n",
      "\n",
      "==neck_speed==\n",
      "MSE: 0.04471\n",
      "MAE: 0.15034\n",
      "Example [Regen-Real]: 0.19990191 - 0.1019247938021254\n",
      "\n",
      "==l_hand_acceleration_magnitude==\n",
      "MSE: 0.00000\n",
      "MAE: 0.00117\n",
      "Example [Regen-Real]: 1.8147573 - 1.8147300283008208\n",
      "\n",
      "==r_hand_acceleration_magnitude==\n",
      "MSE: 0.72308\n",
      "MAE: 0.56622\n",
      "Example [Regen-Real]: 0.26055798 - 0.1515197113567625\n",
      "\n",
      "==l_foot_acceleration_magnitude==\n",
      "MSE: 0.15557\n",
      "MAE: 0.25937\n",
      "Example [Regen-Real]: 0.42629468 - 0.0942168827817646\n",
      "\n",
      "==r_foot_acceleration_magnitude==\n",
      "MSE: 0.16694\n",
      "MAE: 0.26889\n",
      "Example [Regen-Real]: 0.43840444 - 0.5786308173366547\n",
      "\n",
      "==neck_acceleration_magnitude==\n",
      "MSE: 0.19345\n",
      "MAE: 0.30818\n",
      "Example [Regen-Real]: 1.2749867 - 0.3167880836245241\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mae_errors = mean_absolute_error(x_test, decoded_lma, multioutput='raw_values')\n",
    "mse_errors = mean_squared_error(x_test, decoded_lma, multioutput='raw_values')\n",
    "\n",
    "features = [\"max_hand_distance\",\n",
    "          \"avg_l_hand_hip_distance\",\n",
    "          \"avg_r_hand_hip_distance\",\n",
    "          \"max_stride_length\",\n",
    "          \"avg_l_hand_chest_distance\",\n",
    "          \"avg_r_hand_chest_distance\",\n",
    "          \"avg_l_elbow_hip_distance\",\n",
    "          \"avg_r_elbow_hip_distance\",\n",
    "          \"avg_chest_pelvis_distance\",\n",
    "          \"avg_neck_chest_distance\",\n",
    "          \"avg_neck_rotation_w\", \"avg_neck_rotation_x\", \"avg_neck_rotation_y\", \"avg_neck_rotation_z\",\n",
    "          \"avg_total_body_volume\",\n",
    "          \"avg_triangle_area_hands_neck\",\n",
    "          \"avg_triangle_area_feet_hips\",\n",
    "          \n",
    "          \"l_hand_speed\",\n",
    "          \"r_hand_speed\",\n",
    "          \"l_foot_speed\",\n",
    "          \"r_foot_speed\",\n",
    "          \"neck_speed\",\n",
    "          \n",
    "          \"l_hand_acceleration_magnitude\",\n",
    "          \"r_hand_acceleration_magnitude\",\n",
    "          \"l_foot_acceleration_magnitude\",\n",
    "          \"r_foot_acceleration_magnitude\",\n",
    "          \"neck_acceleration_magnitude\",\n",
    "         ]\n",
    "\n",
    "print(\"Overall MAE: \" + str(mean_absolute_error(x_test, decoded_lma)))\n",
    "\n",
    "print()\n",
    "for i in range(len(mse_errors)):\n",
    "    print(\"==\" + features[i] + \"==\")\n",
    "    print(\"MSE: %.5f\" % mse_errors[i])\n",
    "    print(\"MAE: %.5f\" % mae_errors[i])\n",
    "    print(\"Example [Regen-Real]: \" + str(decoded_lma[i][i]) + \" - \" + str(x_test[i][i]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_p = xgb.XGBRegressor(verbosity=0)\n",
    "model_p.load_model(\"../../emotion_classifier/model_training/models/l2p_dance_model_O.json\")\n",
    "\n",
    "model_a = xgb.XGBRegressor(verbosity=0)\n",
    "model_a.load_model(\"../../emotion_classifier/model_training/models/l2a_dance_model_O.json\")\n",
    "\n",
    "model_d = xgb.XGBRegressor(verbosity=0)\n",
    "model_d.load_model(\"../../emotion_classifier/model_training/models/l2d_dance_model_O.json\")\n",
    "\n",
    "scaler = joblib.load('../../emotion_classifier/model_training/datasets/scalers/standardizers/Fs_B_O_S_DANCE_WALK_KIN_0.5sec.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.6037664   0.53833523  0.49170665  0.28366809  0.35884839  0.33160644\n",
      "   0.36780567  0.371009    0.286151    0.27881744 -0.03755763 -0.10271722\n",
      "   0.02814101  0.99303877  0.2768524   0.05992045  0.11764969  0.92542711\n",
      "   0.84483981  0.36246837  0.4541739   0.27634474  1.66896815  1.20506391\n",
      "   0.69472364  0.79142185  0.46329054]]\n"
     ]
    }
   ],
   "source": [
    "sample = np.asarray(x_test[4])\n",
    "sample = sample.reshape(1,-1)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.3121054 0.        1.3494374]]\n"
     ]
    }
   ],
   "source": [
    "generated = encoder.predict(sample)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.45192295  0.27009922  0.28957313  0.3135463   0.4351236   0.44136572\n",
      "   0.3281574   0.33616108  0.28609994  0.27865002  0.00609086 -0.11397311\n",
      "  -0.05980234  0.9808791   0.21628958  0.11374643  0.13001555  0.17562008\n",
      "   0.21498972  0.15963885  0.16807863  0.1755821   0.3314573   0.46309248\n",
      "   0.42756844  0.45085353  0.38934135]]\n"
     ]
    }
   ],
   "source": [
    "regen = decoder.predict(generated)\n",
    "print(regen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real: [0.5817592, 0.49667734, 0.18971871]\n",
      "Predicted: [-0.30992487, 0.09826839, -0.15161158]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diogosilva/.local/lib/python3.7/site-packages/sklearn/base.py:451: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "/home/diogosilva/.local/lib/python3.7/site-packages/sklearn/base.py:451: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n"
     ]
    }
   ],
   "source": [
    "scaled_sample = scaler.transform(sample)\n",
    "\n",
    "real_coordinates = (\n",
    "    model_p.predict(scaled_sample),\n",
    "    model_a.predict(scaled_sample),\n",
    "    model_d.predict(scaled_sample)\n",
    ")\n",
    "\n",
    "scaled_regen = scaler.transform(regen)\n",
    "\n",
    "generated_coordinates = (\n",
    "    model_p.predict(scaled_regen),\n",
    "    model_a.predict(scaled_regen),\n",
    "    model_d.predict(scaled_regen)\n",
    ")\n",
    "\n",
    "\n",
    "print('Real: %s' % [real_coordinates[0][0], real_coordinates[1][0], real_coordinates[2][0]])\n",
    "print('Predicted: %s' % [generated_coordinates[0][0], generated_coordinates[1][0], generated_coordinates[2][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_dict = {\n",
    "    (-0.5, 0.6, 0.9): \"angry\",\n",
    "    (0.6, 0.5, 0.2): \"happy\",\n",
    "    (-0.6, -0.3, -0.3): \"sad\",\n",
    "    (-0.4, 0.25, -0.1): \"disgusted\",\n",
    "    (-0.35, 0.7, -0.8): \"afraid\",\n",
    "    (0.7, 0.2, 0.2): \"pleased\",\n",
    "    (-0.5, -0.7, -0.25): \"bored\",\n",
    "    (0.1, -0.7, -0.2): \"tired\",\n",
    "    (0.6, -0.55, 0.1): \"relaxed\",\n",
    "    (0.5, 0.7, 0.4): \"excited\",\n",
    "    (-0.85, -0.1, -0.8): \"miserable\",\n",
    "    (-0.3, -0.66, -0.7): \"nervous\",\n",
    "    (0.9, -0.25, 0.65): \"satisfied\",   \n",
    "}\n",
    "\n",
    "colour_dict = {\n",
    "    \"angry\": \"crimson\",\n",
    "    \"happy\": \"springgreen\",\n",
    "    \"sad\": \"cornflowerblue\",\n",
    "    \"disgusted\": \"darkorange\"  ,\n",
    "    \"afraid\": \"gold\",\n",
    "    \"pleased\": \"olive\",\n",
    "    \"bored\": \"lightseagreen\",\n",
    "    \"tired\": \"plum\",\n",
    "    \"relaxed\": \"chocolate\",\n",
    "    \"excited\": \"olivedrab\",\n",
    "    \"miserable\": \"purple\",\n",
    "    \"nervous\": \"lightslategray\",\n",
    "    \"satisfied\": \"lightpink\",   \n",
    "}\n",
    "\n",
    "x_test_encoded = encoder.predict(x_test, batch_size=32)\n",
    "plt.figure(figsize=(6, 6))\n",
    "test_y = []\n",
    "for i in range(len(test_emotions)):\n",
    "    point_coords = (test_emotions.iloc[i][0], test_emotions.iloc[i][1], test_emotions.iloc[i][2])\n",
    "    \n",
    "    test_y.append(colour_dict[conv_dict[point_coords]])\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=test_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PAD to Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_p = xgb.XGBRegressor(verbosity=0)\n",
    "model_p.load_model(\"../../emotion_classifier/model_training/models/l2p_dance_model_O.json\")\n",
    "\n",
    "model_a = xgb.XGBRegressor(verbosity=0)\n",
    "model_a.load_model(\"../../emotion_classifier/model_training/models/l2a_dance_model_O.json\")\n",
    "\n",
    "model_d = xgb.XGBRegressor(verbosity=0)\n",
    "model_d.load_model(\"../../emotion_classifier/model_training/models/l2d_dance_model_O.json\")\n",
    "\n",
    "scaler = joblib.load('../../emotion_classifier/model_training/datasets/scalers/standardizers/Fs_B_O_S_DANCE_WALK_KIN_0.5sec.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X -> Predicted emotions of LMA features\n",
    "# y -> Latent Space of LMA features\n",
    "# Good Error: Predicted emotion of LMA features generated from latent space == Predicted emotions of original LMA features\n",
    "\n",
    "dataset = pd.read_csv('../../emotion_classifier/model_training/datasets/Fs_B_O_S_DANCE_WALK_KIN_0.5sec.csv')\n",
    "\n",
    "train_dataset = dataset.sample(frac=0.85, random_state=42)\n",
    "test_dataset = dataset.drop(train_dataset.index)\n",
    "\n",
    "print(\"No Training Samples:\",train_dataset.shape[0])\n",
    "print(\"No Test Samples:\",test_dataset.shape[0])\n",
    "\n",
    "train_lma = train_dataset.copy()\n",
    "test_lma = test_dataset.copy()\n",
    "\n",
    "train_emotions = pd.concat([train_lma.pop(x) for x in ['EMOTION_P', 'EMOTION_A', 'EMOTION_D']], axis=1)\n",
    "test_emotions = pd.concat([test_lma.pop(x) for x in ['EMOTION_P', 'EMOTION_A', 'EMOTION_D']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_p = model_p.predict(train_lma)\n",
    "train_X_a = model_a.predict(train_lma)\n",
    "train_X_d = model_d.predict(train_lma)\n",
    "\n",
    "rows = []\n",
    "for i in range(len(train_lma)):\n",
    "    rows.append([train_X_p[i], train_X_a[i], train_X_d[i]])\n",
    "\n",
    "train_X = pd.DataFrame(rows, columns=[\n",
    "            \"EMOTION_P\", \"EMOTION_A\", \"EMOTION_D\"\n",
    "         ])\n",
    "\n",
    "train_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X_p = model_p.predict(test_lma)\n",
    "test_X_a = model_a.predict(test_lma)\n",
    "test_X_d = model_d.predict(test_lma)\n",
    "\n",
    "rows = []\n",
    "for i in range(len(test_lma)):\n",
    "    rows.append([test_X_p[i], test_X_a[i], test_X_d[i]])\n",
    "\n",
    "test_X = pd.DataFrame(rows, columns=[\n",
    "            \"EMOTION_P\", \"EMOTION_A\", \"EMOTION_D\"\n",
    "         ])\n",
    "\n",
    "test_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_space = encoder.predict(train_lma)\n",
    "\n",
    "rows = []\n",
    "for i in range(len(latent_space)):\n",
    "    rows.append([latent_space[i][0], latent_space[i][1], latent_space[i][2]])\n",
    "    \n",
    "train_y = pd.DataFrame(rows, columns=[\n",
    "            \"LATENT_1\", \"LATENT_2\" , \"LATENT_3\"\n",
    "         ])\n",
    "\n",
    "train_y_1 = pd.concat([train_y.pop(x) for x in ['LATENT_1']], axis=1)\n",
    "train_y_2 = pd.concat([train_y.pop(x) for x in ['LATENT_2']], axis=1)\n",
    "train_y_3 = pd.concat([train_y.pop(x) for x in ['LATENT_3']], axis=1)\n",
    "\n",
    "train_y_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_space = encoder.predict(test_lma)\n",
    "\n",
    "rows = []\n",
    "for i in range(len(latent_space)):\n",
    "    rows.append([latent_space[i][0], latent_space[i][1], latent_space[i][2]])\n",
    "    \n",
    "test_y = pd.DataFrame(rows, columns=[\n",
    "            \"LATENT_1\", \"LATENT_2\" , \"LATENT_3\"\n",
    "         ])\n",
    "\n",
    "test_y_1 = pd.concat([test_y.pop(x) for x in ['LATENT_1']], axis=1)\n",
    "test_y_2 = pd.concat([test_y.pop(x) for x in ['LATENT_2']], axis=1)\n",
    "test_y_3 = pd.concat([test_y.pop(x) for x in ['LATENT_3']], axis=1)\n",
    "\n",
    "test_y_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = xgb.XGBRegressor(\n",
    "                    n_estimators=1500, learning_rate=0.1, max_depth=15, min_child_weight=1, \n",
    "                    reg_alpha=0.25, reg_lambda=1.25, gamma=0.001,\n",
    "                    subsample=0.75, colsample_bytree=1.0, objective=\"reg:squarederror\",\n",
    "                    tree_method='gpu_hist'\n",
    "                )\n",
    "\n",
    "\n",
    "model_2 = xgb.XGBRegressor(\n",
    "                    n_estimators=1500, learning_rate=0.05, max_depth=15, min_child_weight=1, \n",
    "                    reg_alpha=0, reg_lambda=1.25, gamma=0.001,\n",
    "                    subsample=0.75, colsample_bytree=1.0, objective=\"reg:squarederror\",\n",
    "                    tree_method='gpu_hist'\n",
    "                )\n",
    "\n",
    "\n",
    "model_3 = xgb.XGBRegressor(\n",
    "                    n_estimators=1500, learning_rate=0.05, max_depth=15, min_child_weight=1, \n",
    "                    reg_alpha=0.0, reg_lambda=1, gamma=0.0,\n",
    "                    subsample=0.75, colsample_bytree=1.0, objective=\"reg:squarederror\",\n",
    "                    tree_method='gpu_hist'\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.fit(train_X, train_y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.fit(train_X, train_y_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.fit(train_X, train_y_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model_1.score(train_X, train_y_1)  \n",
    "\n",
    "print(\"Latent 1\")\n",
    "print(\"Training score: \", score)\n",
    "\n",
    "print()\n",
    "\n",
    "score = model_2.score(train_X, train_y_2)  \n",
    "\n",
    "print(\"Latent 2\")\n",
    "print(\"Training score: \", score)\n",
    "\n",
    "score = model_3.score(train_X, train_y_3)  \n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Latent 3\")\n",
    "print(\"Training score: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y_1 = model_1.predict(test_X)\n",
    "mse = mean_squared_error(test_y_1, pred_y_1)\n",
    "mae = mean_absolute_error(test_y_1, pred_y_1)\n",
    "print(\"Latent 1\")\n",
    "print(\"MSE: %.2f\" % mse)\n",
    "print(\"MAE: %.2f\" % mae)\n",
    "\n",
    "\n",
    "pred_y_2 = model_2.predict(test_X)\n",
    "mse = mean_squared_error(test_y_2, pred_y_2)\n",
    "mae = mean_absolute_error(test_y_2, pred_y_2)\n",
    "print(\"\\nLatent 2\")\n",
    "print(\"MSE: %.2f\" % mse)\n",
    "print(\"MAE: %.2f\" % mae)\n",
    "\n",
    "\n",
    "pred_y_3 = model_3.predict(test_X)\n",
    "mse = mean_squared_error(test_y_3, pred_y_3)\n",
    "mae = mean_absolute_error(test_y_3, pred_y_3)\n",
    "print(\"\\nLatent 3\")\n",
    "print(\"MSE: %.2f\" % mse)\n",
    "print(\"MAE: %.2f\" % mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 3\n",
    "\n",
    "# Real -> PAD coordinates\n",
    "#   - get latent space\n",
    "#   - convert from latent to LMA features\n",
    "#   - standardize\n",
    "#   - predict coordinates of generated features\n",
    "\n",
    "sample = np.asarray([test_X.iloc[index]])\n",
    "latent = np.asarray([[model_1.predict(sample)[0], model_2.predict(sample)[0], model_3.predict(sample)[0]]])\n",
    "\n",
    "\n",
    "generated_lma = decoder.predict(latent)\n",
    "\n",
    "\n",
    "scaled_gen = scaler.transform(generated_lma)\n",
    "\n",
    "generated_coord = (\n",
    "    model_p.predict(scaled_gen)[0],\n",
    "    model_a.predict(scaled_gen)[0],\n",
    "    model_d.predict(scaled_gen)[0]\n",
    ")\n",
    "\n",
    "print('Real: %s' % sample[0])\n",
    "print('Predicted: %s' % np.asarray(generated_coord))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A parameter grid for XGBoost\n",
    "# https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "params = {\n",
    "        'eta': [0.01, 0.05, 0.1],\n",
    "        'min_child_weight': [1, 5, 11, 21],\n",
    "        'max_depth': [3, 6, 10, 15],\n",
    "        'gamma': [0, 0.001, 0.01],\n",
    "        'subsample': [0.75, 1],\n",
    "        'colsample_bytree': [0.75, 1],\n",
    "        'lambda': [1, 1.25],\n",
    "        'alpha': [0.0, 0.25]\n",
    "        }\n",
    "\n",
    "n_iter = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = xgb.XGBRegressor(\n",
    "                    n_estimators=1500,\n",
    "                    objective=\"reg:squarederror\",\n",
    "                    tree_method='gpu_hist'\n",
    "                )\n",
    "\n",
    "model_2 = xgb.XGBRegressor(\n",
    "                    n_estimators=1500,\n",
    "                    objective=\"reg:squarederror\",\n",
    "                    tree_method='gpu_hist'\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent 1\n",
    "# run randomized search\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "random_search_1 = RandomizedSearchCV(model_1, param_distributions=params,\n",
    "                               cv=kfold, scoring='neg_mean_squared_error', n_iter = n_iter)\n",
    "\n",
    "start = time.time()\n",
    "random_search_1.fit(train_X, train_y_1)\n",
    "\n",
    "\n",
    "print(\"GridSearchCV took %.2f seconds\"\n",
    "      \" parameter settings.\" % ((time.time() - start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent 2\n",
    "# run randomized search\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "random_search_2 = RandomizedSearchCV(model_2, param_distributions=params,\n",
    "                               cv=kfold, scoring='neg_mean_squared_error', n_iter = n_iter)\n",
    "\n",
    "start = time.time()\n",
    "random_search_2.fit(train_X, train_y_2)\n",
    "\n",
    "\n",
    "print(\"GridSearchCV took %.2f seconds\"\n",
    "      \" parameter settings.\" % ((time.time() - start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent 3\n",
    "# run randomized search\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "random_search_3 = RandomizedSearchCV(model_3, param_distributions=params,\n",
    "                               cv=kfold, scoring='neg_mean_squared_error', n_iter = n_iter)\n",
    "\n",
    "start = time.time()\n",
    "random_search_3.fit(train_X, train_y_3)\n",
    "\n",
    "\n",
    "print(\"GridSearchCV took %.2f seconds\"\n",
    "      \" parameter settings.\" % ((time.time() - start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_regressor_1 = random_search_1.best_estimator_\n",
    "\n",
    "print(best_regressor_1.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_regressor_2 = random_search_2.best_estimator_\n",
    "\n",
    "print(best_regressor_2.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_regressor_3 = random_search_3.best_estimator_\n",
    "\n",
    "print(best_regressor_3.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y_1 = best_regressor_1.predict(test_X)\n",
    "mse = mean_squared_error(test_y_1, pred_y_1)\n",
    "mae = mean_absolute_error(test_y_1, pred_y_1)\n",
    "print(\"Latent 1\")\n",
    "print(\"MSE: %.2f\" % mse)\n",
    "print(\"MAE: %.2f\" % mae)\n",
    "\n",
    "\n",
    "pred_y_2 = best_regressor_2.predict(test_X)\n",
    "mse = mean_squared_error(test_y_2, pred_y_2)\n",
    "mae = mean_absolute_error(test_y_2, pred_y_2)\n",
    "print(\"\\nLatent 2\")\n",
    "print(\"MSE: %.2f\" % mse)\n",
    "print(\"MAE: %.2f\" % mae)\n",
    "\n",
    "pred_y_3 = best_regressor_3.predict(test_X)\n",
    "mse = mean_squared_error(test_y_3, pred_y_3)\n",
    "mae = mean_absolute_error(test_y_3, pred_y_3)\n",
    "print(\"\\nLatent 3\")\n",
    "print(\"MSE: %.2f\" % mse)\n",
    "print(\"MAE: %.2f\" % mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
