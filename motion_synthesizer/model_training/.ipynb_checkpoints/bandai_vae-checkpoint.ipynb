{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "xgb.set_config(verbosity=0)\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_p = xgb.XGBRegressor(verbosity=0)\n",
    "model_p.load_model(\"../../emotion_classifier/model_training/models/l2p_dance_model_Fs2_O.json\")\n",
    "\n",
    "model_a = xgb.XGBRegressor(verbosity=0)\n",
    "model_a.load_model(\"../../emotion_classifier/model_training/models/l2a_dance_model_Fs2_O.json\")\n",
    "\n",
    "model_d = xgb.XGBRegressor(verbosity=0)\n",
    "model_d.load_model(\"../../emotion_classifier/model_training/models/l2d_dance_model_Fs2_O.json\")\n",
    "\n",
    "scaler = joblib.load('../../emotion_classifier/model_training/datasets/scalers/standardizers/Fs2_B_O_S_DANCE_WALK_KIN_0.5sec.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_hand_distance</th>\n",
       "      <th>avg_l_hand_hip_distance</th>\n",
       "      <th>avg_r_hand_hip_distance</th>\n",
       "      <th>max_stride_length</th>\n",
       "      <th>avg_l_hand_chest_distance</th>\n",
       "      <th>avg_r_hand_chest_distance</th>\n",
       "      <th>avg_l_elbow_hip_distance</th>\n",
       "      <th>avg_r_elbow_hip_distance</th>\n",
       "      <th>avg_neck_chest_distance</th>\n",
       "      <th>avg_neck_rotation_w</th>\n",
       "      <th>...</th>\n",
       "      <th>r_foot_speed</th>\n",
       "      <th>neck_speed</th>\n",
       "      <th>l_hand_acceleration_magnitude</th>\n",
       "      <th>r_hand_acceleration_magnitude</th>\n",
       "      <th>l_foot_acceleration_magnitude</th>\n",
       "      <th>r_foot_acceleration_magnitude</th>\n",
       "      <th>neck_acceleration_magnitude</th>\n",
       "      <th>EMOTION_P</th>\n",
       "      <th>EMOTION_A</th>\n",
       "      <th>EMOTION_D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.684605</td>\n",
       "      <td>0.268051</td>\n",
       "      <td>0.236534</td>\n",
       "      <td>0.478955</td>\n",
       "      <td>0.469381</td>\n",
       "      <td>0.455152</td>\n",
       "      <td>0.351238</td>\n",
       "      <td>0.336392</td>\n",
       "      <td>0.278741</td>\n",
       "      <td>0.011497</td>\n",
       "      <td>...</td>\n",
       "      <td>0.531111</td>\n",
       "      <td>0.677884</td>\n",
       "      <td>1.109843</td>\n",
       "      <td>0.766334</td>\n",
       "      <td>1.537713</td>\n",
       "      <td>1.062221</td>\n",
       "      <td>1.355767</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.438911</td>\n",
       "      <td>0.265254</td>\n",
       "      <td>0.229588</td>\n",
       "      <td>0.283025</td>\n",
       "      <td>0.468163</td>\n",
       "      <td>0.452005</td>\n",
       "      <td>0.350459</td>\n",
       "      <td>0.333469</td>\n",
       "      <td>0.278750</td>\n",
       "      <td>0.011885</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014436</td>\n",
       "      <td>0.021416</td>\n",
       "      <td>1.085836</td>\n",
       "      <td>0.779991</td>\n",
       "      <td>1.575538</td>\n",
       "      <td>1.049393</td>\n",
       "      <td>1.367237</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.440199</td>\n",
       "      <td>0.266441</td>\n",
       "      <td>0.223630</td>\n",
       "      <td>0.309053</td>\n",
       "      <td>0.455287</td>\n",
       "      <td>0.439872</td>\n",
       "      <td>0.343657</td>\n",
       "      <td>0.325378</td>\n",
       "      <td>0.278739</td>\n",
       "      <td>0.017740</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072819</td>\n",
       "      <td>0.191442</td>\n",
       "      <td>1.231576</td>\n",
       "      <td>1.117785</td>\n",
       "      <td>0.133194</td>\n",
       "      <td>0.126177</td>\n",
       "      <td>0.355767</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.500120</td>\n",
       "      <td>0.386881</td>\n",
       "      <td>0.393727</td>\n",
       "      <td>0.452718</td>\n",
       "      <td>0.418828</td>\n",
       "      <td>0.440395</td>\n",
       "      <td>0.324313</td>\n",
       "      <td>0.340373</td>\n",
       "      <td>0.278729</td>\n",
       "      <td>0.033718</td>\n",
       "      <td>...</td>\n",
       "      <td>0.464108</td>\n",
       "      <td>0.605356</td>\n",
       "      <td>1.068333</td>\n",
       "      <td>1.038694</td>\n",
       "      <td>1.638293</td>\n",
       "      <td>1.021144</td>\n",
       "      <td>1.332992</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.562528</td>\n",
       "      <td>0.447009</td>\n",
       "      <td>0.596923</td>\n",
       "      <td>0.488571</td>\n",
       "      <td>0.361697</td>\n",
       "      <td>0.522971</td>\n",
       "      <td>0.286162</td>\n",
       "      <td>0.430045</td>\n",
       "      <td>0.278747</td>\n",
       "      <td>0.041425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.139125</td>\n",
       "      <td>0.207208</td>\n",
       "      <td>1.304347</td>\n",
       "      <td>1.077684</td>\n",
       "      <td>1.563165</td>\n",
       "      <td>0.917321</td>\n",
       "      <td>0.803440</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   max_hand_distance  avg_l_hand_hip_distance  avg_r_hand_hip_distance  \\\n",
       "0           0.684605                 0.268051                 0.236534   \n",
       "1           0.438911                 0.265254                 0.229588   \n",
       "2           0.440199                 0.266441                 0.223630   \n",
       "3           0.500120                 0.386881                 0.393727   \n",
       "4           0.562528                 0.447009                 0.596923   \n",
       "\n",
       "   max_stride_length  avg_l_hand_chest_distance  avg_r_hand_chest_distance  \\\n",
       "0           0.478955                   0.469381                   0.455152   \n",
       "1           0.283025                   0.468163                   0.452005   \n",
       "2           0.309053                   0.455287                   0.439872   \n",
       "3           0.452718                   0.418828                   0.440395   \n",
       "4           0.488571                   0.361697                   0.522971   \n",
       "\n",
       "   avg_l_elbow_hip_distance  avg_r_elbow_hip_distance  \\\n",
       "0                  0.351238                  0.336392   \n",
       "1                  0.350459                  0.333469   \n",
       "2                  0.343657                  0.325378   \n",
       "3                  0.324313                  0.340373   \n",
       "4                  0.286162                  0.430045   \n",
       "\n",
       "   avg_neck_chest_distance  avg_neck_rotation_w  ...  r_foot_speed  \\\n",
       "0                 0.278741             0.011497  ...      0.531111   \n",
       "1                 0.278750             0.011885  ...      0.014436   \n",
       "2                 0.278739             0.017740  ...      0.072819   \n",
       "3                 0.278729             0.033718  ...      0.464108   \n",
       "4                 0.278747             0.041425  ...      0.139125   \n",
       "\n",
       "   neck_speed  l_hand_acceleration_magnitude  r_hand_acceleration_magnitude  \\\n",
       "0    0.677884                       1.109843                       0.766334   \n",
       "1    0.021416                       1.085836                       0.779991   \n",
       "2    0.191442                       1.231576                       1.117785   \n",
       "3    0.605356                       1.068333                       1.038694   \n",
       "4    0.207208                       1.304347                       1.077684   \n",
       "\n",
       "   l_foot_acceleration_magnitude  r_foot_acceleration_magnitude  \\\n",
       "0                       1.537713                       1.062221   \n",
       "1                       1.575538                       1.049393   \n",
       "2                       0.133194                       0.126177   \n",
       "3                       1.638293                       1.021144   \n",
       "4                       1.563165                       0.917321   \n",
       "\n",
       "   neck_acceleration_magnitude  EMOTION_P  EMOTION_A  EMOTION_D  \n",
       "0                     1.355767       -0.5        0.6        0.9  \n",
       "1                     1.367237       -0.5        0.6        0.9  \n",
       "2                     0.355767       -0.5        0.6        0.9  \n",
       "3                     1.332992       -0.5        0.6        0.9  \n",
       "4                     0.803440       -0.5        0.6        0.9  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('datasets/Fs3_B_O_DANCE_WALK_KIN_0.5sec.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Training Samples: 38538\n",
      "No Test Samples: 2028\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset.sample(frac=0.95, random_state=42)\n",
    "test_dataset = dataset.drop(train_dataset.index)\n",
    "\n",
    "print(\"No Training Samples:\",train_dataset.shape[0])\n",
    "print(\"No Test Samples:\",test_dataset.shape[0])\n",
    "\n",
    "train_dataset = shuffle(train_dataset)\n",
    "test_dataset = shuffle(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_emotions = pd.concat([train_dataset.pop(x) for x in ['EMOTION_P', 'EMOTION_A', 'EMOTION_D']], axis=1)\n",
    "train_emotions_OG = train_emotions.copy()\n",
    "\n",
    "test_emotions = pd.concat([test_dataset.pop(x) for x in ['EMOTION_P', 'EMOTION_A', 'EMOTION_D']], axis=1)\n",
    "test_emotions_OG = test_emotions.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMOTION_P</th>\n",
       "      <th>EMOTION_A</th>\n",
       "      <th>EMOTION_D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.170779</td>\n",
       "      <td>0.420426</td>\n",
       "      <td>0.074570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.316093</td>\n",
       "      <td>0.395268</td>\n",
       "      <td>0.191429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.290567</td>\n",
       "      <td>0.417218</td>\n",
       "      <td>0.108208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.487052</td>\n",
       "      <td>0.309305</td>\n",
       "      <td>0.258323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.279108</td>\n",
       "      <td>0.477474</td>\n",
       "      <td>0.098426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   EMOTION_P  EMOTION_A  EMOTION_D\n",
       "0   0.170779   0.420426   0.074570\n",
       "1   0.316093   0.395268   0.191429\n",
       "2   0.290567   0.417218   0.108208\n",
       "3   0.487052   0.309305   0.258323\n",
       "4   0.279108   0.477474   0.098426"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_emotions_p = model_p.predict(train_dataset)\n",
    "train_emotions_a = model_a.predict(train_dataset)\n",
    "train_emotions_d = model_d.predict(train_dataset)\n",
    "\n",
    "rows = []\n",
    "for i in range(len(train_dataset)):\n",
    "    rows.append([train_emotions_p[i], train_emotions_a[i], train_emotions_d[i]])\n",
    "\n",
    "train_emotions = pd.DataFrame(rows, columns=[\n",
    "            \"EMOTION_P\", \"EMOTION_A\", \"EMOTION_D\"\n",
    "         ])\n",
    "\n",
    "train_emotions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMOTION_P</th>\n",
       "      <th>EMOTION_A</th>\n",
       "      <th>EMOTION_D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.442142</td>\n",
       "      <td>0.469476</td>\n",
       "      <td>0.213168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.103089</td>\n",
       "      <td>0.413134</td>\n",
       "      <td>0.224867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.276700</td>\n",
       "      <td>0.364156</td>\n",
       "      <td>0.130408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.013347</td>\n",
       "      <td>0.405878</td>\n",
       "      <td>0.231400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.203966</td>\n",
       "      <td>0.427358</td>\n",
       "      <td>0.117642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   EMOTION_P  EMOTION_A  EMOTION_D\n",
       "0   0.442142   0.469476   0.213168\n",
       "1  -0.103089   0.413134   0.224867\n",
       "2   0.276700   0.364156   0.130408\n",
       "3  -0.013347   0.405878   0.231400\n",
       "4   0.203966   0.427358   0.117642"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_emotions_p = model_p.predict(test_dataset)\n",
    "test_emotions_a = model_a.predict(test_dataset)\n",
    "test_emotions_d = model_d.predict(test_dataset)\n",
    "\n",
    "rows = []\n",
    "for i in range(len(test_dataset)):\n",
    "    rows.append([test_emotions_p[i], test_emotions_a[i], test_emotions_d[i]])\n",
    "\n",
    "test_emotions = pd.DataFrame(rows, columns=[\n",
    "            \"EMOTION_P\", \"EMOTION_A\", \"EMOTION_D\"\n",
    "         ])\n",
    "\n",
    "test_emotions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(input_vols, output_vols):\n",
    "    beta = 1e-7\n",
    "    kl_loss = K.sum(-1 - K.log(K.exp(log_var)) + K.exp(log_var) + K.square(mu))/2\n",
    "    return K.mean((input_vols-output_vols)**2) + beta*kl_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_37 (InputLayer)          [(None, 26)]         0           []                               \n",
      "                                                                                                  \n",
      " dense_108 (Dense)              (None, 16)           432         ['input_37[0][0]']               \n",
      "                                                                                                  \n",
      " dense_109 (Dense)              (None, 8)            136         ['dense_108[0][0]']              \n",
      "                                                                                                  \n",
      " dense_110 (Dense)              (None, 4)            36          ['dense_109[0][0]']              \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 4)            20          ['dense_110[0][0]']              \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 4)            20          ['dense_110[0][0]']              \n",
      "                                                                                                  \n",
      " sampling_18 (Sampling)         (None, 4)            0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 644\n",
      "Trainable params: 644\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 4\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(26, ))\n",
    "x = layers.Dense(16, activation=\"relu\")(encoder_inputs)\n",
    "x = layers.Dense(8, activation=\"relu\")(x)\n",
    "x = layers.Dense(4, activation=\"relu\")(x)\n",
    "\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_38 (InputLayer)       [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_111 (Dense)           (None, 4)                 20        \n",
      "                                                                 \n",
      " dense_112 (Dense)           (None, 8)                 40        \n",
      "                                                                 \n",
      " dense_113 (Dense)           (None, 16)                144       \n",
      "                                                                 \n",
      " dense_114 (Dense)           (None, 26)                442       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 646\n",
      "Trainable params: 646\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(4, activation=\"relu\")(latent_inputs)\n",
    "x = layers.Dense(8, activation=\"relu\")(x)\n",
    "x = layers.Dense(16, activation=\"relu\")(x)\n",
    "\n",
    "decoder_outputs = layers.Dense((26, )[0], activation=\"linear\")(x)\n",
    "\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        \n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            \n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = keras.losses.mean_squared_error(data, reconstruction)\n",
    "            \n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            \n",
    "            total_loss = (50 * reconstruction_loss) + (kl_loss)\n",
    "            \n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        \n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/256\n",
      "7708/7708 [==============================] - 10s 1ms/step - loss: 8.7512 - reconstruction_loss: 0.1118 - kl_loss: 1.2754\n",
      "Epoch 2/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 6.1713 - reconstruction_loss: 0.0983 - kl_loss: 1.2626\n",
      "Epoch 3/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 6.1648 - reconstruction_loss: 0.0979 - kl_loss: 1.2628\n",
      "Epoch 4/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 6.1931 - reconstruction_loss: 0.0976 - kl_loss: 1.2656\n",
      "Epoch 5/256\n",
      "7708/7708 [==============================] - 10s 1ms/step - loss: 6.1367 - reconstruction_loss: 0.0975 - kl_loss: 1.2649\n",
      "Epoch 6/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 6.0765 - reconstruction_loss: 0.0975 - kl_loss: 1.2562\n",
      "Epoch 7/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 6.1384 - reconstruction_loss: 0.0973 - kl_loss: 1.2534\n",
      "Epoch 8/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 6.1178 - reconstruction_loss: 0.0973 - kl_loss: 1.2618\n",
      "Epoch 9/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 6.1698 - reconstruction_loss: 0.0972 - kl_loss: 1.2653\n",
      "Epoch 10/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 6.1234 - reconstruction_loss: 0.0972 - kl_loss: 1.2611\n",
      "Epoch 11/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 6.1225 - reconstruction_loss: 0.0970 - kl_loss: 1.2627\n",
      "Epoch 12/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 6.1158 - reconstruction_loss: 0.0970 - kl_loss: 1.2656\n",
      "Epoch 13/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 6.1352 - reconstruction_loss: 0.0969 - kl_loss: 1.2601\n",
      "Epoch 14/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 6.1651 - reconstruction_loss: 0.0968 - kl_loss: 1.2743\n",
      "Epoch 15/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 6.0896 - reconstruction_loss: 0.0967 - kl_loss: 1.2601\n",
      "Epoch 16/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 6.0989 - reconstruction_loss: 0.0967 - kl_loss: 1.2678\n",
      "Epoch 17/256\n",
      "7708/7708 [==============================] - 10s 1ms/step - loss: 6.1251 - reconstruction_loss: 0.0965 - kl_loss: 1.2731\n",
      "Epoch 18/256\n",
      "7708/7708 [==============================] - 10s 1ms/step - loss: 6.0786 - reconstruction_loss: 0.0964 - kl_loss: 1.2725\n",
      "Epoch 19/256\n",
      "7708/7708 [==============================] - 10s 1ms/step - loss: 6.0754 - reconstruction_loss: 0.0963 - kl_loss: 1.2759\n",
      "Epoch 20/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 6.0577 - reconstruction_loss: 0.0961 - kl_loss: 1.2729\n",
      "Epoch 21/256\n",
      "7708/7708 [==============================] - 10s 1ms/step - loss: 6.0730 - reconstruction_loss: 0.0958 - kl_loss: 1.2863\n",
      "Epoch 22/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 6.0655 - reconstruction_loss: 0.0957 - kl_loss: 1.2973\n",
      "Epoch 23/256\n",
      "7708/7708 [==============================] - 10s 1ms/step - loss: 6.0891 - reconstruction_loss: 0.0954 - kl_loss: 1.3100\n",
      "Epoch 24/256\n",
      "7708/7708 [==============================] - 10s 1ms/step - loss: 6.1026 - reconstruction_loss: 0.0951 - kl_loss: 1.3178\n",
      "Epoch 25/256\n",
      "7708/7708 [==============================] - 10s 1ms/step - loss: 6.0842 - reconstruction_loss: 0.0948 - kl_loss: 1.3177\n",
      "Epoch 26/256\n",
      "7708/7708 [==============================] - 10s 1ms/step - loss: 6.0672 - reconstruction_loss: 0.0946 - kl_loss: 1.3303\n",
      "Epoch 27/256\n",
      "7708/7708 [==============================] - 10s 1ms/step - loss: 6.0721 - reconstruction_loss: 0.0944 - kl_loss: 1.3337\n",
      "Epoch 28/256\n",
      "7708/7708 [==============================] - 10s 1ms/step - loss: 6.0062 - reconstruction_loss: 0.0940 - kl_loss: 1.3417\n",
      "Epoch 29/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.9914 - reconstruction_loss: 0.0939 - kl_loss: 1.3447\n",
      "Epoch 30/256\n",
      "7708/7708 [==============================] - 10s 1ms/step - loss: 5.9737 - reconstruction_loss: 0.0934 - kl_loss: 1.3615\n",
      "Epoch 31/256\n",
      "7708/7708 [==============================] - 10s 1ms/step - loss: 6.0261 - reconstruction_loss: 0.0933 - kl_loss: 1.3642\n",
      "Epoch 32/256\n",
      "7708/7708 [==============================] - 10s 1ms/step - loss: 6.0509 - reconstruction_loss: 0.0934 - kl_loss: 1.3767\n",
      "Epoch 33/256\n",
      "7708/7708 [==============================] - 10s 1ms/step - loss: 5.9494 - reconstruction_loss: 0.0931 - kl_loss: 1.3823\n",
      "Epoch 34/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 6.0492 - reconstruction_loss: 0.0929 - kl_loss: 1.3675\n",
      "Epoch 35/256\n",
      "7708/7708 [==============================] - 10s 1ms/step - loss: 6.0250 - reconstruction_loss: 0.0932 - kl_loss: 1.3677\n",
      "Epoch 36/256\n",
      "7708/7708 [==============================] - 10s 1ms/step - loss: 6.0373 - reconstruction_loss: 0.0925 - kl_loss: 1.4007\n",
      "Epoch 37/256\n",
      "7708/7708 [==============================] - 10s 1ms/step - loss: 5.9749 - reconstruction_loss: 0.0922 - kl_loss: 1.3909\n",
      "Epoch 38/256\n",
      "7708/7708 [==============================] - 10s 1ms/step - loss: 5.9979 - reconstruction_loss: 0.0922 - kl_loss: 1.4073\n",
      "Epoch 39/256\n",
      "7708/7708 [==============================] - 10s 1ms/step - loss: 6.0153 - reconstruction_loss: 0.0928 - kl_loss: 1.3846\n",
      "Epoch 40/256\n",
      "7708/7708 [==============================] - 10s 1ms/step - loss: 5.9976 - reconstruction_loss: 0.0925 - kl_loss: 1.3941\n",
      "Epoch 41/256\n",
      "7708/7708 [==============================] - 10s 1ms/step - loss: 6.0051 - reconstruction_loss: 0.0915 - kl_loss: 1.4221\n",
      "Epoch 42/256\n",
      "7708/7708 [==============================] - 10s 1ms/step - loss: 6.0099 - reconstruction_loss: 0.0917 - kl_loss: 1.4328\n",
      "Epoch 43/256\n",
      "7708/7708 [==============================] - 10s 1ms/step - loss: 6.0822 - reconstruction_loss: 0.0917 - kl_loss: 1.4336\n",
      "Epoch 44/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 6.0369 - reconstruction_loss: 0.0919 - kl_loss: 1.4117\n",
      "Epoch 45/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 6.0038 - reconstruction_loss: 0.0931 - kl_loss: 1.3935\n",
      "Epoch 46/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 6.0704 - reconstruction_loss: 0.0928 - kl_loss: 1.4020\n",
      "Epoch 47/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 6.0879 - reconstruction_loss: 0.0920 - kl_loss: 1.4284\n",
      "Epoch 48/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 6.0844 - reconstruction_loss: 0.0942 - kl_loss: 1.3733\n",
      "Epoch 49/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 6.0927 - reconstruction_loss: 0.0935 - kl_loss: 1.4431\n",
      "Epoch 50/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 6.1369 - reconstruction_loss: 0.0925 - kl_loss: 1.4504\n",
      "Epoch 51/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 6.0358 - reconstruction_loss: 0.0911 - kl_loss: 1.4654\n",
      "Epoch 52/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 6.0274 - reconstruction_loss: 0.0910 - kl_loss: 1.4690\n",
      "Epoch 53/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.9886 - reconstruction_loss: 0.0919 - kl_loss: 1.4384\n",
      "Epoch 54/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 6.0059 - reconstruction_loss: 0.0916 - kl_loss: 1.4537\n",
      "Epoch 55/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 6.0354 - reconstruction_loss: 0.0907 - kl_loss: 1.4787\n",
      "Epoch 56/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 6.1302 - reconstruction_loss: 0.0952 - kl_loss: 1.3803\n",
      "Epoch 57/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 6.1384 - reconstruction_loss: 0.0934 - kl_loss: 1.4350\n",
      "Epoch 58/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 6.1148 - reconstruction_loss: 0.0932 - kl_loss: 1.4253\n",
      "Epoch 59/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 6.0356 - reconstruction_loss: 0.0922 - kl_loss: 1.4229\n",
      "Epoch 60/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 6.0671 - reconstruction_loss: 0.0948 - kl_loss: 1.3625\n",
      "Epoch 61/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 6.0492 - reconstruction_loss: 0.0917 - kl_loss: 1.4620\n",
      "Epoch 62/256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7708/7708 [==============================] - 10s 1ms/step - loss: 6.0608 - reconstruction_loss: 0.0920 - kl_loss: 1.4797\n",
      "Epoch 63/256\n",
      "7708/7708 [==============================] - 10s 1ms/step - loss: 6.0337 - reconstruction_loss: 0.0916 - kl_loss: 1.4589\n",
      "Epoch 64/256\n",
      "7708/7708 [==============================] - 10s 1ms/step - loss: 6.0437 - reconstruction_loss: 0.0912 - kl_loss: 1.4494\n",
      "Epoch 65/256\n",
      "7708/7708 [==============================] - 10s 1ms/step - loss: 6.0917 - reconstruction_loss: 0.0924 - kl_loss: 1.4674\n",
      "Epoch 66/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 6.1503 - reconstruction_loss: 0.0944 - kl_loss: 1.3953\n",
      "Epoch 67/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 6.1509 - reconstruction_loss: 0.0941 - kl_loss: 1.3778\n",
      "Epoch 68/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 6.0701 - reconstruction_loss: 0.0937 - kl_loss: 1.4318\n",
      "Epoch 69/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 6.0806 - reconstruction_loss: 0.0937 - kl_loss: 1.4231\n",
      "Epoch 70/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 6.0246 - reconstruction_loss: 0.0928 - kl_loss: 1.4355\n",
      "Epoch 71/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.9599 - reconstruction_loss: 0.0921 - kl_loss: 1.4392\n",
      "Epoch 72/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.9747 - reconstruction_loss: 0.0910 - kl_loss: 1.4285\n",
      "Epoch 73/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 6.0128 - reconstruction_loss: 0.0904 - kl_loss: 1.4502\n",
      "Epoch 74/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.9025 - reconstruction_loss: 0.0856 - kl_loss: 1.5430\n",
      "Epoch 75/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.7429 - reconstruction_loss: 0.0818 - kl_loss: 1.5977\n",
      "Epoch 76/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.6758 - reconstruction_loss: 0.0805 - kl_loss: 1.6379\n",
      "Epoch 77/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.6562 - reconstruction_loss: 0.0799 - kl_loss: 1.6410\n",
      "Epoch 78/256\n",
      "7708/7708 [==============================] - 10s 1ms/step - loss: 5.6362 - reconstruction_loss: 0.0793 - kl_loss: 1.6635\n",
      "Epoch 79/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.6292 - reconstruction_loss: 0.0792 - kl_loss: 1.6714\n",
      "Epoch 80/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.6190 - reconstruction_loss: 0.0791 - kl_loss: 1.6708\n",
      "Epoch 81/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.6139 - reconstruction_loss: 0.0788 - kl_loss: 1.6718\n",
      "Epoch 82/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5933 - reconstruction_loss: 0.0785 - kl_loss: 1.6761\n",
      "Epoch 83/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.6211 - reconstruction_loss: 0.0783 - kl_loss: 1.6801\n",
      "Epoch 84/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5871 - reconstruction_loss: 0.0779 - kl_loss: 1.7063\n",
      "Epoch 85/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5941 - reconstruction_loss: 0.0775 - kl_loss: 1.7101\n",
      "Epoch 86/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5784 - reconstruction_loss: 0.0773 - kl_loss: 1.7134\n",
      "Epoch 87/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5830 - reconstruction_loss: 0.0769 - kl_loss: 1.7177\n",
      "Epoch 88/256\n",
      "7708/7708 [==============================] - 10s 1ms/step - loss: 5.5384 - reconstruction_loss: 0.0767 - kl_loss: 1.7285\n",
      "Epoch 89/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5245 - reconstruction_loss: 0.0765 - kl_loss: 1.7348\n",
      "Epoch 90/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5422 - reconstruction_loss: 0.0764 - kl_loss: 1.7379\n",
      "Epoch 91/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5078 - reconstruction_loss: 0.0764 - kl_loss: 1.7395\n",
      "Epoch 92/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5965 - reconstruction_loss: 0.0762 - kl_loss: 1.7500\n",
      "Epoch 93/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5349 - reconstruction_loss: 0.0760 - kl_loss: 1.7472\n",
      "Epoch 94/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5652 - reconstruction_loss: 0.0759 - kl_loss: 1.7453\n",
      "Epoch 95/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5370 - reconstruction_loss: 0.0758 - kl_loss: 1.7565\n",
      "Epoch 96/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5284 - reconstruction_loss: 0.0759 - kl_loss: 1.7437\n",
      "Epoch 97/256\n",
      "7708/7708 [==============================] - 10s 1ms/step - loss: 5.5537 - reconstruction_loss: 0.0760 - kl_loss: 1.7477\n",
      "Epoch 98/256\n",
      "7708/7708 [==============================] - 10s 1ms/step - loss: 5.5735 - reconstruction_loss: 0.0761 - kl_loss: 1.7431\n",
      "Epoch 99/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5404 - reconstruction_loss: 0.0760 - kl_loss: 1.7408\n",
      "Epoch 100/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5253 - reconstruction_loss: 0.0758 - kl_loss: 1.7554\n",
      "Epoch 101/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5359 - reconstruction_loss: 0.0757 - kl_loss: 1.7504\n",
      "Epoch 102/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5109 - reconstruction_loss: 0.0758 - kl_loss: 1.7439\n",
      "Epoch 103/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5527 - reconstruction_loss: 0.0759 - kl_loss: 1.7471\n",
      "Epoch 104/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5335 - reconstruction_loss: 0.0758 - kl_loss: 1.7573\n",
      "Epoch 105/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5430 - reconstruction_loss: 0.0758 - kl_loss: 1.7539\n",
      "Epoch 106/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5289 - reconstruction_loss: 0.0756 - kl_loss: 1.7497\n",
      "Epoch 107/256\n",
      "7708/7708 [==============================] - 10s 1ms/step - loss: 5.5514 - reconstruction_loss: 0.0758 - kl_loss: 1.7525\n",
      "Epoch 108/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5368 - reconstruction_loss: 0.0757 - kl_loss: 1.7597\n",
      "Epoch 109/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5330 - reconstruction_loss: 0.0756 - kl_loss: 1.7556\n",
      "Epoch 110/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5852 - reconstruction_loss: 0.0757 - kl_loss: 1.7478\n",
      "Epoch 111/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5030 - reconstruction_loss: 0.0756 - kl_loss: 1.7691\n",
      "Epoch 112/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5535 - reconstruction_loss: 0.0758 - kl_loss: 1.7532\n",
      "Epoch 113/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5173 - reconstruction_loss: 0.0757 - kl_loss: 1.7493\n",
      "Epoch 114/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5100 - reconstruction_loss: 0.0757 - kl_loss: 1.7604\n",
      "Epoch 115/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5775 - reconstruction_loss: 0.0756 - kl_loss: 1.7631\n",
      "Epoch 116/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5037 - reconstruction_loss: 0.0755 - kl_loss: 1.7575\n",
      "Epoch 117/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5436 - reconstruction_loss: 0.0757 - kl_loss: 1.7576\n",
      "Epoch 118/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5132 - reconstruction_loss: 0.0755 - kl_loss: 1.7551\n",
      "Epoch 119/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5026 - reconstruction_loss: 0.0754 - kl_loss: 1.7553\n",
      "Epoch 120/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5174 - reconstruction_loss: 0.0756 - kl_loss: 1.7478\n",
      "Epoch 121/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5652 - reconstruction_loss: 0.0755 - kl_loss: 1.7516\n",
      "Epoch 122/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5344 - reconstruction_loss: 0.0754 - kl_loss: 1.7513\n",
      "Epoch 123/256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5501 - reconstruction_loss: 0.0753 - kl_loss: 1.7734\n",
      "Epoch 124/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5522 - reconstruction_loss: 0.0754 - kl_loss: 1.7616\n",
      "Epoch 125/256\n",
      "7708/7708 [==============================] - 10s 1ms/step - loss: 5.5246 - reconstruction_loss: 0.0754 - kl_loss: 1.7527\n",
      "Epoch 126/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5270 - reconstruction_loss: 0.0753 - kl_loss: 1.7634\n",
      "Epoch 127/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5199 - reconstruction_loss: 0.0753 - kl_loss: 1.7684\n",
      "Epoch 128/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5377 - reconstruction_loss: 0.0752 - kl_loss: 1.7701\n",
      "Epoch 129/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.4825 - reconstruction_loss: 0.0752 - kl_loss: 1.7771\n",
      "Epoch 130/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5091 - reconstruction_loss: 0.0751 - kl_loss: 1.7707\n",
      "Epoch 131/256\n",
      "7708/7708 [==============================] - 10s 1ms/step - loss: 5.5246 - reconstruction_loss: 0.0751 - kl_loss: 1.7657\n",
      "Epoch 132/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5000 - reconstruction_loss: 0.0751 - kl_loss: 1.7691\n",
      "Epoch 133/256\n",
      "7708/7708 [==============================] - 10s 1ms/step - loss: 5.5581 - reconstruction_loss: 0.0749 - kl_loss: 1.7806\n",
      "Epoch 134/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5051 - reconstruction_loss: 0.0749 - kl_loss: 1.7768\n",
      "Epoch 135/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5393 - reconstruction_loss: 0.0747 - kl_loss: 1.7752\n",
      "Epoch 136/256\n",
      "7708/7708 [==============================] - 10s 1ms/step - loss: 5.5307 - reconstruction_loss: 0.0746 - kl_loss: 1.7882\n",
      "Epoch 137/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.4985 - reconstruction_loss: 0.0746 - kl_loss: 1.7922\n",
      "Epoch 138/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.4912 - reconstruction_loss: 0.0745 - kl_loss: 1.7820\n",
      "Epoch 139/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5217 - reconstruction_loss: 0.0745 - kl_loss: 1.7830\n",
      "Epoch 140/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5237 - reconstruction_loss: 0.0744 - kl_loss: 1.7931\n",
      "Epoch 141/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.4759 - reconstruction_loss: 0.0742 - kl_loss: 1.7887\n",
      "Epoch 142/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.4995 - reconstruction_loss: 0.0743 - kl_loss: 1.8016\n",
      "Epoch 143/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5139 - reconstruction_loss: 0.0743 - kl_loss: 1.7964\n",
      "Epoch 144/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.4816 - reconstruction_loss: 0.0742 - kl_loss: 1.7985\n",
      "Epoch 145/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5150 - reconstruction_loss: 0.0741 - kl_loss: 1.8027\n",
      "Epoch 146/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.4737 - reconstruction_loss: 0.0741 - kl_loss: 1.8016\n",
      "Epoch 147/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5290 - reconstruction_loss: 0.0742 - kl_loss: 1.8056\n",
      "Epoch 148/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.4928 - reconstruction_loss: 0.0741 - kl_loss: 1.7994\n",
      "Epoch 149/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.4877 - reconstruction_loss: 0.0740 - kl_loss: 1.8074\n",
      "Epoch 150/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5048 - reconstruction_loss: 0.0740 - kl_loss: 1.7979\n",
      "Epoch 151/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5145 - reconstruction_loss: 0.0740 - kl_loss: 1.8013\n",
      "Epoch 152/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5246 - reconstruction_loss: 0.0738 - kl_loss: 1.8094\n",
      "Epoch 153/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5245 - reconstruction_loss: 0.0740 - kl_loss: 1.8046\n",
      "Epoch 154/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.4667 - reconstruction_loss: 0.0739 - kl_loss: 1.8075\n",
      "Epoch 155/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5033 - reconstruction_loss: 0.0739 - kl_loss: 1.8044\n",
      "Epoch 156/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5232 - reconstruction_loss: 0.0738 - kl_loss: 1.8146\n",
      "Epoch 157/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5134 - reconstruction_loss: 0.0738 - kl_loss: 1.8028\n",
      "Epoch 158/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5123 - reconstruction_loss: 0.0738 - kl_loss: 1.8127\n",
      "Epoch 159/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.4806 - reconstruction_loss: 0.0740 - kl_loss: 1.8033\n",
      "Epoch 160/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5005 - reconstruction_loss: 0.0738 - kl_loss: 1.8048\n",
      "Epoch 161/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5063 - reconstruction_loss: 0.0737 - kl_loss: 1.8042\n",
      "Epoch 162/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.4934 - reconstruction_loss: 0.0737 - kl_loss: 1.8106\n",
      "Epoch 163/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.4817 - reconstruction_loss: 0.0737 - kl_loss: 1.8067\n",
      "Epoch 164/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5135 - reconstruction_loss: 0.0734 - kl_loss: 1.8229\n",
      "Epoch 165/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5085 - reconstruction_loss: 0.0736 - kl_loss: 1.8213\n",
      "Epoch 166/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5171 - reconstruction_loss: 0.0738 - kl_loss: 1.8107\n",
      "Epoch 167/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5285 - reconstruction_loss: 0.0736 - kl_loss: 1.8077\n",
      "Epoch 168/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5447 - reconstruction_loss: 0.0737 - kl_loss: 1.8156\n",
      "Epoch 169/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.4452 - reconstruction_loss: 0.0735 - kl_loss: 1.8113\n",
      "Epoch 170/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5182 - reconstruction_loss: 0.0735 - kl_loss: 1.8171\n",
      "Epoch 171/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5158 - reconstruction_loss: 0.0736 - kl_loss: 1.8163\n",
      "Epoch 172/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.4887 - reconstruction_loss: 0.0735 - kl_loss: 1.8177\n",
      "Epoch 173/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.4886 - reconstruction_loss: 0.0734 - kl_loss: 1.8308\n",
      "Epoch 174/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.4805 - reconstruction_loss: 0.0733 - kl_loss: 1.8222\n",
      "Epoch 175/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5229 - reconstruction_loss: 0.0736 - kl_loss: 1.8117\n",
      "Epoch 176/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.4612 - reconstruction_loss: 0.0734 - kl_loss: 1.8121\n",
      "Epoch 177/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.4810 - reconstruction_loss: 0.0734 - kl_loss: 1.8227\n",
      "Epoch 178/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5242 - reconstruction_loss: 0.0735 - kl_loss: 1.8297\n",
      "Epoch 179/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5130 - reconstruction_loss: 0.0734 - kl_loss: 1.8229\n",
      "Epoch 180/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.4853 - reconstruction_loss: 0.0733 - kl_loss: 1.8248\n",
      "Epoch 181/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5116 - reconstruction_loss: 0.0733 - kl_loss: 1.8239\n",
      "Epoch 182/256\n",
      "7708/7708 [==============================] - 8s 1ms/step - loss: 5.4643 - reconstruction_loss: 0.0733 - kl_loss: 1.8237\n",
      "Epoch 183/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.4869 - reconstruction_loss: 0.0736 - kl_loss: 1.8080\n",
      "Epoch 184/256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.4788 - reconstruction_loss: 0.0735 - kl_loss: 1.8195\n",
      "Epoch 185/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.4724 - reconstruction_loss: 0.0732 - kl_loss: 1.8248\n",
      "Epoch 186/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.4713 - reconstruction_loss: 0.0733 - kl_loss: 1.8225\n",
      "Epoch 187/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.4442 - reconstruction_loss: 0.0733 - kl_loss: 1.8250\n",
      "Epoch 188/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5055 - reconstruction_loss: 0.0733 - kl_loss: 1.8331\n",
      "Epoch 189/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.4997 - reconstruction_loss: 0.0734 - kl_loss: 1.8254\n",
      "Epoch 190/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.4867 - reconstruction_loss: 0.0735 - kl_loss: 1.8209\n",
      "Epoch 191/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.4955 - reconstruction_loss: 0.0733 - kl_loss: 1.8250\n",
      "Epoch 192/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.4750 - reconstruction_loss: 0.0732 - kl_loss: 1.8293\n",
      "Epoch 193/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5044 - reconstruction_loss: 0.0732 - kl_loss: 1.8310\n",
      "Epoch 194/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5204 - reconstruction_loss: 0.0733 - kl_loss: 1.8260\n",
      "Epoch 195/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.4742 - reconstruction_loss: 0.0731 - kl_loss: 1.8260\n",
      "Epoch 196/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.4582 - reconstruction_loss: 0.0732 - kl_loss: 1.8252\n",
      "Epoch 197/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.4419 - reconstruction_loss: 0.0730 - kl_loss: 1.8331\n",
      "Epoch 198/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5011 - reconstruction_loss: 0.0732 - kl_loss: 1.8278\n",
      "Epoch 199/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.4845 - reconstruction_loss: 0.0732 - kl_loss: 1.8304\n",
      "Epoch 200/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.4718 - reconstruction_loss: 0.0730 - kl_loss: 1.8346\n",
      "Epoch 201/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.4683 - reconstruction_loss: 0.0731 - kl_loss: 1.8377\n",
      "Epoch 202/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.4666 - reconstruction_loss: 0.0732 - kl_loss: 1.8313\n",
      "Epoch 203/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.4478 - reconstruction_loss: 0.0731 - kl_loss: 1.8286\n",
      "Epoch 204/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5048 - reconstruction_loss: 0.0729 - kl_loss: 1.8440\n",
      "Epoch 205/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.4627 - reconstruction_loss: 0.0734 - kl_loss: 1.8130\n",
      "Epoch 206/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.4862 - reconstruction_loss: 0.0734 - kl_loss: 1.8250\n",
      "Epoch 207/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5074 - reconstruction_loss: 0.0732 - kl_loss: 1.8267\n",
      "Epoch 208/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.4873 - reconstruction_loss: 0.0731 - kl_loss: 1.8305\n",
      "Epoch 209/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.4694 - reconstruction_loss: 0.0733 - kl_loss: 1.8220\n",
      "Epoch 210/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.4909 - reconstruction_loss: 0.0734 - kl_loss: 1.8199\n",
      "Epoch 211/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.4668 - reconstruction_loss: 0.0731 - kl_loss: 1.8314\n",
      "Epoch 212/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.4799 - reconstruction_loss: 0.0731 - kl_loss: 1.8395\n",
      "Epoch 213/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.4625 - reconstruction_loss: 0.0733 - kl_loss: 1.8149\n",
      "Epoch 214/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.5249 - reconstruction_loss: 0.0729 - kl_loss: 1.8419\n",
      "Epoch 215/256\n",
      "7708/7708 [==============================] - 9s 1ms/step - loss: 5.4666 - reconstruction_loss: 0.0732 - kl_loss: 1.8264\n",
      "Epoch 216/256\n",
      "3946/7708 [==============>...............] - ETA: 4s - loss: 5.4637 - reconstruction_loss: 0.0728 - kl_loss: 1.8318"
     ]
    }
   ],
   "source": [
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam())\n",
    "vae.fit(train_dataset, epochs=256, batch_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_dict = {\n",
    "    (-0.5, 0.6, 0.9): \"angry\",\n",
    "    (0.6, 0.5, 0.2): \"happy\",\n",
    "    (-0.6, -0.3, -0.3): \"sad\",\n",
    "    (-0.4, 0.25, -0.1): \"disgusted\",\n",
    "    (-0.35, 0.7, -0.8): \"afraid\",\n",
    "    (0.7, 0.2, 0.2): \"pleased\",\n",
    "    (-0.5, -0.7, -0.25): \"bored\",\n",
    "    (0.1, -0.7, -0.2): \"tired\",\n",
    "    (0.6, -0.55, 0.1): \"relaxed\",\n",
    "    (0.5, 0.7, 0.4): \"excited\",\n",
    "    (-0.85, -0.1, -0.8): \"miserable\",\n",
    "    (-0.3, -0.66, -0.7): \"nervous\",\n",
    "    (0.9, -0.25, 0.65): \"satisfied\",   \n",
    "}\n",
    "\n",
    "colour_dict = {\n",
    "    \"angry\": \"crimson\",\n",
    "    \"happy\": \"springgreen\",\n",
    "    \"sad\": \"cornflowerblue\",\n",
    "    \"disgusted\": \"darkorange\"  ,\n",
    "    \"afraid\": \"gold\",\n",
    "    \"pleased\": \"olive\",\n",
    "    \"bored\": \"lightseagreen\",\n",
    "    \"tired\": \"plum\",\n",
    "    \"relaxed\": \"chocolate\",\n",
    "    \"excited\": \"olivedrab\",\n",
    "    \"miserable\": \"purple\",\n",
    "    \"nervous\": \"lightslategray\",\n",
    "    \"satisfied\": \"lightpink\",   \n",
    "}\n",
    "\n",
    "def plot_label_clusters(vae, data, labels):\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "    z_mean, _, _ = vae.encoder.predict(data)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    actual_labels = []\n",
    "    for i in range(len(labels)):\n",
    "        point_coords = (labels.iloc[i][0], labels.iloc[i][1], labels.iloc[i][2])\n",
    "\n",
    "        actual_labels.append(colour_dict[conv_dict[point_coords]])\n",
    "\n",
    "    \n",
    "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=actual_labels)\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.show()\n",
    "\n",
    "plot_label_clusters(vae, test_dataset, test_emotions_OG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
